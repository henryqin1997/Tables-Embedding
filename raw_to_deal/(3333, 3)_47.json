{"relation": [["triples", "7,186,445", "410,659", "273,644", "237,685", "196,239", "194,730", "178,842", "165,948", "165,506", "160,592"], ["context", "http://sw.deri.org/svn/sw/2008/03/MetaX/vocab.rdf#aperture-1.2.0", "http://lsdis.cs.uga.edu/projects/semdis/swetodblp/march2007/swetodblp_march_2007_part_22.rdf", "http://www.ling.helsinki.fi/kit/2004k/ctl310semw/WordNet/wordnet_nouns-20010201.rdf", "http://dbpedia.org/resource/\u30ea\u30ec\u30c3\u30c8", "http://dbpedia.org/resource/\u30b9\u30b1\u30eb\u30c4\u30a9", "http://dbpedia.org/resource/\u30aa\u30fc\u30af\u30b7\u30e7\u30f3", "http://www.reveredata.com/reports/store/index-companies.rss", "http://lsdis.cs.uga.edu/~satya/Satya/jan24.owl", "http://www.cs.man.ac.uk/~dturi/ontologies/go-assocdb/go-termdb.owl", "http://dbpedia.org/resource/\u3092\u304c\u308f\u3044\u3061\u308d\u3092"]], "pageTitle": "(still) nothing clever \u2014 Visualisation", "title": "", "url": "http://gromgull.net/blog/category/visualisation/", "hasHeader": true, "headerPosition": "FIRST_ROW", "tableType": "RELATION", "tableNum": 2, "s3Link": "common-crawl/crawl-data/CC-MAIN-2015-32/segments/1438042989897.84/warc/CC-MAIN-20150728002309-00057-ip-10-236-191-2.ec2.internal.warc.gz", "recordEndOffset": 105195119, "recordOffset": 105168163, "tableOrientation": "HORIZONTAL", "TableContextTimeStampBeforeTable": "{6417=At http://gromgull.net/2010/10/btc/explore.html you can see the result. Clicking one a namespace will show only hosts publishing triples using this schema, and only schemas that co-occur with the one you picked. Conversely, click on a host will show the namespaces published by that host, and only hosts that use the same schemas (this makes less intuitive sense for hosts than for namespaces). You even get a little protovis histogram of the distribution of hosts/namespaces!, 8665=Posted by gromgull at 11:45 am on February 17th, 2011. 3 comments... \u00bb Categories: Bash, Visualisation., 13631=Yesterday I was made to watch the Eurovision song contest. I went to bed before the voting ended, so I woke up to find that Azerbaijan had won. Which was curious, since they were awful (or perhaps not, since this is Eurovision)., 11230=Posted by gromgull at 7:12 pm on May 15th, 2011. 6 comments... \u00bb Categories: fun, Statistics, Visualisation., 11869=I think the only solution is to go back to the 1960 version of Eurovision, the first year that all countries that matter took part., 2533=Posted by gromgull at 11:00 am on May 16th, 2010. 2 comments... \u00bb Categories: Machine Learning, Python, Visualisation., 5701=Posted by gromgull at 1:39 pm on October 12th, 2010. 3 comments... \u00bb Categories: Billion Triple Challenge, Statistics, Visualisation., 7816=A little while back I spent about 1 CPU week computing which hosts use which namespaces in the BTC2010 data, i.e. I computed a matrix with hosts as rows, schemas as columns and each cell the number of triples using that namespace each host published. My plan was to use this to create a co-occurrence matrix for schemas, and then use this for computing similarities for hierarchical clustering. And I did. And it was not very amazing. Like Ed Summer\u2019s neat LOD graph I wanted to use Protovis to make it pretty. Then, after making one version, uglier than the next I realised that just looking at the clustering tree as a javascript datastructure was just as useful, I gave up on the whole clustering thing., 13349=At the official webpage you can get the voting breakdown, where we can see that all the countries I have lived in (Norway, UK and Germany) gave Azerbaijan 0 points. Clearly the Eurovision has been ruined by all these new East-block counties, who in a giant conspiracy who only vote for each other, rendering us western countries with real musical talent without a chance. To confirm my suspicion I grabbed the result table, python, scipy and matplotlib. Compute the correlation matrix for the columns, run PCA on this and plot the first two components (if all that meant nothing to you, the result is that countries who tend to distribute their votes similarly are close to each other in the diagram):}", "TableContextTimeStampAfterTable": "{9923=Posted by gromgull at 2:10 pm on September 11th, 2009. No comments... \u00bb Categories: Billion Triple Challenge, Python, Semantic Web, Statistics, Visualisation., 4640=Posted by gromgull at 12:25 pm on January 6th, 2010. 2 comments... \u00bb Categories: Billion Triple Challenge, in progress, SVG, Visualisation., 20750=Posted by gromgull at 10:19 pm on August 31st, 2009. 2 comments... \u00bb Categories: Machine Learning, Python, Visualisation., 30792=Posted by gromgull at 12:50 pm on July 16th, 2009. One comment... \u00bb Categories: Billion Triple Challenge, Semantic Web, Statistics, Visualisation., 15636=So instead of FastMap I used maketree from the complearn tools, this generates trees from a distance matrix, it generates very good results, but it is an iterative optimisation and it takes forever for large instances. Around this time I realised I wasn\u2019t going to be able to visualise all 7500 PLDs, and cut it down to the 2000, 1000, 500, 100, 50 largest PLDs. Now this worked fine, but the result looked like a bog-standard graphviz graph, and it wasn\u2019t very exciting (i.e not at all like this colourful thing). Now I realised that since I actually had numeric feature vectors in the first place I wasn\u2019t restrained to using FastMap to make up coordinates, and I used PCA to map the input vector-space to a 3-dimensional space, normalised the values to [0;255] and used these as RGB values for colour. Ah \u2013 lovely pastel., 41747=Posted by gromgull at 8:55 pm on June 25th, 2009. 5 comments... \u00bb Categories: Billion Triple Challenge, R, Semantic Web, Statistics, Visualisation., 17031=Posted by gromgull at 12:02 pm on September 9th, 2009. 3 comments... \u00bb Categories: Billion Triple Challenge, Python, Semantic Web, Statistics, Visualisation., 20458=This is already quite old, from 1995, and I am sure something better exists now, but it\u2019s a nice little thing to have in the toolbox. I wonder if it can be used to estimate numerical feature values for nominal attributes, in cases where all possible values are known?}", "textBeforeTable": "The biggest context were as follows: 3,759 more than 10000 covers 7% 133,369 more than 1000 covers 30% 1,574,458 more than 100\u00a0 covers 63% 10,278,663 yielded more than 10 triples, and covers 85% of the full data. 35,423,929 yielded more than a single triple Time for a few more BTC statistics, this time looking at the contexts. The BTC data comes from 50,207,171 different URLs, out of these: Billions and billions and billions (on a map) Posted by gromgull at 10:19 pm on August 31st, 2009. 2 comments... \u00bb Categories: Machine Learning, Python, Visualisation. This is already quite old, from 1995, and I am sure something better exists now, but it\u2019s a nice little thing to have in the toolbox. I wonder if it can be used to estimate numerical feature values for nominal attributes, in cases where all possible values are known? The distance measure used keeps the projection \u201cin mind\u201d, so the second iteration will be different to the first.The whole thing is a bit like Principal Component Analysis, but without requiring an original matrix of feature vectors. recurse :) the line between these two points becomes the first dimension, project all points to this line", "textAfterTable": "It\u2019s pretty cool that someone crawled 7 million triples with aperture and put it online :) \u2013 the link is 404 now though, so you can\u2019t easily check what it was. Also, none of the huge dbpedia pages seem to give any info, I am not quite sure what is going on there. Perhaps some encoding trouble somewhere? As the official BTC statistics page already shows, it is more interesting when you group the context by the ones with the same host, computing the same Pay-Level-Domains as they did I get the hosts contributing the most triples as: triples context 278,566,771 dbpedia.org 133,266,773 livejournal.com 94,748,441 rkbexplorer.com 84,896,760 geonames.org 61,339,034 mybloglog.com 53,492,284 sioc-project.org 23,970,898 qdos.com 23,745,914 hi5.com 23,459,199 kanzaki.com 17,691,303 rdfabout.com 15,784,386 plode.us 15,208,914 dbtune.org 13,548,946 craigslist.org 10,155,861 l3s.de 10,028,115 opencyc.org Again, this is computed from the whole dataset, not just a subset, but interestingly it differs quite a lot from the \u201cofficial\u201d statistics, in fact, I\u2019ve \u201clost\u201d over 100M triples from dbpedia. I am not sure why this happens, a handful of context URLs where so strange that python\u2019s urlparse module did you produce a hostname, but they only account for about", "hasKeyColumn": false, "keyColumnIndex": -1, "headerRowIndex": 0}