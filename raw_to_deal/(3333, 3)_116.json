{"relation": [["PREC", "\"half\"", "\"single\"", "\"double\"", "\"quad\"", "\"oct\""], ["IEEE-754 Binary Format", "16-bit half-precision.", "Basic 32-bit single precision.", "Basic 64-bit double precision.", "Basic 128-bit quadruple precision.", "256-bit octuple precision."]], "pageTitle": "The GNU Awk User\u2019s Guide: Setting Precision", "title": "", "url": "http://www.gnu.org/software/gawk/manual/html_node/Setting-Precision.html", "hasHeader": true, "headerPosition": "FIRST_ROW", "tableType": "RELATION", "tableNum": 0, "s3Link": "common-crawl/crawl-data/CC-MAIN-2015-32/segments/1438042989042.37/warc/CC-MAIN-20150728002309-00047-ip-10-236-191-2.ec2.internal.warc.gz", "recordEndOffset": 489107162, "recordOffset": 489098960, "tableOrientation": "HORIZONTAL", "textBeforeTable": "gawk uses a global working precision; it does not keep track of the precision or accuracy of individual numbers. Performing an arithmetic operation or calling a built-in function rounds the result to the current working precision. The default working precision is 53 bits, which can be modified using the built-in variable PREC. You can also set the value to one of the pre-defined case-insensitive strings shown in Table 15.3, to emulate an IEEE-754 binary format. 15.4.1 Setting the Working Precision Next: Setting Rounding Mode, Up: Arbitrary Precision Floats \u00a0 [Contents][Index]", "textAfterTable": "Table 15.3: Predefined precision strings for PREC The following example illustrates the effects of changing precision on arithmetic operations: $ gawk -M -v PREC=100 'BEGIN { x = 1.0e-400; print x + 0 > PREC = \"double\"; print x + 0 }' -| 1e-400 -| 0 Binary and decimal precisions are related approximately, according to the formula: prec = 3.322 * dps Here, prec denotes the binary precision (measured in bits) and dps (short for decimal places) is the decimal digits. We can easily calculate how many decimal digits the 53-bit significand of an IEEE double is equivalent to: 53 / 3.322 which is equal to about 15.95. But what does 15.95 digits actually mean? It depends whether you are concerned about how many digits you can rely on, or how many digits you need. It is important to know how many bits it takes to uniquely identify a double-precision value (the C type double). If you want to convert from double to decimal and back to double (e.g., saving a double representing an intermediate result to a file, and later reading it back to restart the computation), then a few more decimal digits are required. 17 digits is generally enough for a double. It can also be important to know what decimal numbers can be uniquely represented with a double. If you want to convert from decimal to double and back again, 15 digits", "hasKeyColumn": true, "keyColumnIndex": 1, "headerRowIndex": 0}