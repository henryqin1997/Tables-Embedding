{"relation": [["Industry Sector", "Manufacturing", "Transportation", "Retail, Catalog Sales", "Retail, Home Shopping", "Media, Pay Per View", "Banking datacenter", "Financial, Credit Card Processing", "Brokerage"], ["Hourly Cost of Downtime", "$28,000", "$90,000", "$90,000", "$113,000", "$1,100,000", "$2,500,000", "$2,600,000", "$6,500,000"]], "pageTitle": "Strategies for Fault-Tolerant Computing", "title": "", "url": "https://technet.microsoft.com/en-us/library/bb742373(d=printer).aspx", "hasHeader": true, "headerPosition": "FIRST_ROW", "tableType": "RELATION", "tableNum": 1, "s3Link": "common-crawl/crawl-data/CC-MAIN-2015-32/segments/1438042989043.35/warc/CC-MAIN-20150728002309-00035-ip-10-236-191-2.ec2.internal.warc.gz", "recordEndOffset": 891967451, "recordOffset": 891945899, "tableOrientation": "HORIZONTAL", "TableContextTimeStampAfterTable": "{55986=Fortunately, Windows-based fault-tolerant solutions carry far lower costs than proprietary solutions, enabling companies in all industries to achieve a positive ROI in a reasonable timeframe across a much broader range of scenarios. And because support for fault-tolerant servers in Windows 2000 Advanced Server is implemented in a method that makes it transparent to applications, companies embracing fault-tolerance on Windows as a means of achieving mission-critical availability can expect to realize all the other benefits inherent to the Microsoft platform, including reduced time to market, ease of integration, and simplified management., 58174=http://www.microsoft.com/windows2000/community/centers/clustering/default.asp, 38224=However, there are scenarios when using these technologies may not be feasible or appropriate\u0097or when an enterprise needs to further improve system availability by eliminating the potential for downtime due to hardware failure. To help address these situations, Microsoft designed the Windows 2000 Advanced Server operating system to fully support fault-tolerant servers., 42323=Stratus monitors the availability level of every server connected to its service network. According to Stratus, the average availability level for all Windows-based ftServer systems monitored through the network is 99.9998 percent3\u0097a measurement that considers both hardware- and operating system-related downtime. Not only does this validate the effectiveness of fault-tolerant servers, but it confirms that Windows 2000 Advanced Server can deliver mission-critical availability out-of-the-box when used with the proper technologies, managed by well-trained people, and supported with solid processes., 41197=Downtime for Windows 2000-based solutions is typically due to hardware failures, bad device drivers, user error, poor change control processes, and so on, with a very small percentage attributable to the core operating system. In addition to delivering the full \u201ctechnology stack\u201d required to minimize downtime (e.g.; fault-tolerant hardware, a highly stable operating system, hardened device drivers, etc.), vendors of fault-tolerant servers for Windows also offer comprehensive training and service offerings-the three components of the people, process, and technology equation required to maintain a highly available system., 43166=Fault-tolerance on Windows is a compelling solution for companies that need to achieve mission-critical availability but want to avoid expensive and proprietary solutions. Unlike other fault-tolerant platforms that may require \u201ccheckpointing\u201d at the application level, support for fault-tolerance in Windows 2000 Advanced Server is handled completely at in the kernel and hardware abstraction layer\u0097a method that makes it transparent to applications., 51908=Companies should evaluate their total cost of ownership over a five-year period to determine the value that fault-tolerance can provide. For example, in a manufacturing scenario with an existing availability level of three nines, the cost to achieve five nines availability is likely to be far less than the estimated $277,2005 reduction in downtime-related costs over the solution\u2019s five-year lifecycle. This calculation assumes that the system operates 8 hours per day Monday-Friday; if it needs to be available 24x7x365, then the savings due to reduced downtime increases to $1.2 million., 58006=http://www.microsoft.com/windows2000/technologies/clustering/, 36314=Extremely high hardware costs. Use of fault-tolerant servers has traditionally been limited to niche markets, forcing hardware vendors to amortize engineering and manufacturing costs over a small number of units. Prior to 2000, the typical cost for an entry-level fault-tolerant server running a proprietary operating system was $250,000., 57798=Failover Clustering Information on the clustering capabilities in Windows 2000 Advanced Server and Windows 2000 Datacenter Server may be found at:, 37665=Combined with the time to market, productivity, integration, and cost benefits of the Microsoft platform, reliability improvements in the Windows 2000 Server family of operating systems are compelling more and more businesses to deploy Windows-based solutions for their mission-critical computing needs. In these situations, companies often use software-based high-availability technologies included in Windows (e.g., message queuing, distributed transactions, failover clustering, and software load balancing) to minimize unplanned downtime., 38612=The following companies offer integrated solutions for fault-tolerant computing based on Windows 2000 Advanced Server. Microsoft is working closely with these companies to ensure the delivery of integrated hardware, software, and service offerings that make fault-tolerance on Windows a cost-effective alternative to more expensive, proprietary solutions., 40340=Regardless of the reasons for deploying fault-tolerant servers running Windows in any given situation, the platform\u2019s rapid adoption across a broad range of scenarios\u0097and in conservative industry sectors\u0097is compelling evidence of its ability to deliver uncompromised availability. According to Stratus Technologies, which first began shipping fault-tolerant servers running a proprietary operating system in 1982 and added a UNIX-based offering in 1995, the company\u2019s two-year-old Windows-based ftServer product line already accounts for 79 percent all new business.2 NEC Corporation, which began shipping mainframes running proprietary operating systems in 1965, reports similar findings since the introduction of its FT Series fault-tolerant servers for Windows in early 2001.}", "textBeforeTable": "As illustrated in Table 1, availability is typically measured in \u201cnines\u201d. For example, a solution with an availability level of \u201cthree nines\u201d is capable of supporting its intended function 99.9 percent of the time-equivalent to an annual downtime of 8.76 hours per year on a 24x7x365 basis. In the IT community, availability is defined as the percentage of time that a system is capable of serving its intended function. Unlike reliability metrics, which are best used to measure the probability of failure for a single solution component, a solution\u2019s availability level measures the percentage of time that it remains \u201cup and running\u201d in support of an end-user or IT-enabled business process. Therefore, the reliability of all solution components-server hardware, operating system, application software, networking, and so on-can affect a solution\u2019s availability. Availability Defined Introduction Introduction Fault-tolerant Servers Fault Tolerance on Windows Unique Benefits Recommendations For More Information On This Page This document examines the role that fault-tolerant servers can play in solutions that require superior levels of availability. The unique benefits of fault-tolerant servers running the Windows operating system are also discussed, followed by recommendations on when to consider fault-tolerant servers for Windows-based solutions. Abstract Published: January 22, 2003 Strategies", "textAfterTable": "$90,000 Retail, Catalog Sales $90,000 Retail, Home Shopping $113,000 Media, Pay Per View $1,100,000 Banking datacenter $2,500,000 Financial, Credit Card Processing $2,600,000 Brokerage $6,500,000 Table 2: Average Cost of Unplanned Downtime for Various Industries1 However, not all downtime is equally costly; the greatest expense is caused by unplanned downtime. Outside of a system\u2019s core service hours, its amount of downtime\u0097and corresponding overall availability level\u0097may have little to no impact on a business. If a system crashes during core service hours, the result can be significant financial impact. Because unplanned downtime is rarely predictable and can occur at any time, companies looking to minimize risk should evaluate the cost of unplanned downtime during core service hours. For example, in projecting the consequences of planned vs. unplanned downtime, consider a manufacturing scenario where a system is only used during plant hours. Within these intervals, each hour of unplanned downtime costs the company an average of $28,000. However, on evenings and weekends, the system can be taken offline for maintenance or application upgrades with no impact to the company\u2019s operations. Therefore, while the system\u2019s overall availability level may only be two nines, the corresponding 87.6 hours of annual downtime will not have a financial impact (other than possibly paying an", "hasKeyColumn": true, "keyColumnIndex": 0, "headerRowIndex": 0}