{
    "relation": [
        [
            "Variable",
            "I",
            "J",
            "\u03c0",
            "c[i]",
            "\u03b80[j]",
            "\u03b81[j]",
            "\u03b10/(\u03b10+\u03b20)",
            "\u03b10 + \u03b20",
            "\u03b11/(\u03b11+\u03b21)",
            "\u03b11 + \u03b21",
            "x[i,j]"
        ],
        [
            "Range",
            "> 0",
            "> 0",
            "[0,1]",
            "{0,1}",
            "[0,1]",
            "[0,1]",
            "[0,1]",
            "(0,\u221e)",
            "[0,1]",
            "(0,\u221e)",
            "{0,1}"
        ],
        [
            "Status",
            "input",
            "input",
            "estimated",
            "estimated",
            "estimated",
            "estimated",
            "estimated",
            "estimated",
            "estimated",
            "estimated",
            "input"
        ],
        [
            "Distribution",
            "fixed",
            "fixed",
            "Beta(1,1)",
            "Bern(\u03c0)",
            "Beta(\u03b10,\u03b20)",
            "Beta(\u03b11,\u03b21)",
            "Beta(1,1)",
            "Pareto(1.5)*",
            "Beta(1,1)",
            "Pareto(1.5)*",
            "Bern(c[i,j]==1 ? \u03b81[j] : 1-\u03b80[j])"
        ],
        [
            "Description",
            "number of Items",
            "number of annotators",
            "prevalence of category 1",
            "category for item i",
            "specificity of annotator j",
            "sensitivity of annotator j",
            "prior specificity mean",
            "prior specificity scale",
            "prior sensitivity mean",
            "prior sensitivity scale",
            "annotation of item i by annotator j"
        ]
    ],
    "pageTitle": "Collapsed Gibbs Sampler for Hierarchical Annotation Model | LingPipe Blog",
    "title": "",
    "url": "http://lingpipe-blog.com/2009/07/06/collapsed-gibbs-sampler-for-hierarchical-annotation-model/",
    "hasHeader": true,
    "headerPosition": "FIRST_ROW",
    "tableType": "RELATION",
    "tableNum": 0,
    "s3Link": "common-crawl/crawl-data/CC-MAIN-2015-32/segments/1438043062723.96/warc/CC-MAIN-20150728002422-00146-ip-10-236-191-2.ec2.internal.warc.gz",
    "recordEndOffset": 147819119,
    "recordOffset": 147803073,
    "tableOrientation": "HORIZONTAL",
    "TableContextTimeStampAfterTable": "{13535=This entry was posted on July 6, 2009 at 2:51 pm and is filed under Carp's Blog, Data Annotation, LingPipe in Use, LingPipe News. You can follow any responses to this entry through the RSS 2.0 feed. You can leave a response, or trackback from your own site.}",
    "textBeforeTable": "The model\u2019s quite simple, at least in the binomial case. I\u2019ve further simplified here to the case where every annotator labels every item, but the general case is just as easy (modulo indexing): Hierarchical Model of Binomial Annotation Mitzi was away over the weekend, so I naturally spent my newly found \u201cfree time\u201d coding and reading up on stats. While I was procrastinating refactoring feature extraction for CRFs reading a really neat paper (On Smoothing and Inference for Topic Models) from the Irvine paper mill (I also blogged about another of their paper\u2019s on fast LDA sampling), it occurred to me that I could create a fast collapsed sampler for the multinomial form of the hierarchical models of annotation I\u2019ve blogged about. The R and BUGS combination is fine as far as it goes, but it\u2019s slow, hard to debug, and doesn\u2019t scale well. Because I\u2019m going to need to process some big Turker-derived named entity corpora in the next month (more about that later), I thought I\u2019d work on scaling the sampler. by Bob Carpenter Collapsed Gibbs Sampler for Hierarchical Annotation\u00a0Model Deleting Values in Welford\u2019s Algorithm for Online Mean and\u00a0Variance \u00bb Medical Subject Headings (MeSH)\u00a0Parser \u00ab",
    "textAfterTable": "The Collapsed Sampler The basic idea is to sample only the category assignments c[i] in each round of Gibbs sampling. With categories given, it\u2019s easy to compute prevalence, annotator sensitivity and specificity given their conjugate priors. The only thing we need to sample is c[i], and we can inspect the graphical model for dependencies: the parent \u03c0 of the c[i], and the parents \u03b80 and \u03b81 of the descendants x[i,j] of c[i]. The formula\u2019s straightforwardly derived with Bayes\u2019 rule: p(c[i]|x, \u03b80, \u03b81) \u221d p(c[i]) * \u03a0j in 1:J p(x[i,j] | c[i], \u03b80[j], \u03b81[j]) Moment-Matching Beta Priors *The only trick is estimating the priors over the sensitivities and specificities, for which I took the usual expedient of using moment matching. Note that this does not take into account the Pareto prior on scales of the prior specificity and sensitivity (hence the asterisk in the table). In particular, given a set of annotator specificities (and there were 200+ annotators for the named-entity data), we find the beta prior with mean matching the empirical mean and variance matching the empirical variance (requires some algebra). I\u2019m not too worried about the Pareto scale prior \u2014 it\u2019s pretty diffuse. I suppose I could\u2019ve done something with maximum likelihood rather than moment matching (but for all I know, this is the maximum likelihood solution! [update: it\u2019s not the ML estimate; check out Thomas Minka\u2019s paper Estimating a Dirichlet Distribution and references therein.]). Initialization The inputs are initial values for annotator specificity, annotator",
    "hasKeyColumn": true,
    "keyColumnIndex": 4,
    "headerRowIndex": 0
}