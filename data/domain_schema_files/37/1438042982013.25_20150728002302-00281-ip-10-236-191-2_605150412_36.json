{
    "relation": [
        [
            "Topic",
            "Overall satisfaction with NERSC",
            "Available Computing Hardware",
            "Account Support Services",
            "Available software",
            "Network Connectivity",
            "Consulting Overall",
            "HPCF Website",
            "SW maintenance and configuration",
            "HW management and configuration",
            "Consult: Timely response",
            "Mass Storage Facilities",
            "Consult: Technical advice",
            "Allocation Process",
            "Consult: Time to solve problems",
            "SP Overall",
            "SP Uptime",
            "SP Batch Wait Time",
            "Consult: Followup to initial questions",
            "SP Queue Structure",
            "SP User Environment",
            "SP Ability to Run Interactively",
            "SP Disk Configuration and I/O Performance",
            "SP Fortran Compilers",
            "New User's Guide",
            "HPSS Overall",
            "SP Libraries",
            "HPSS User Interface",
            "Consult: Response to special requests",
            "HPSS Performance",
            "HPSS Reliability",
            "HPSS Uptime",
            "NERSC Online Tutorials",
            "SP Performance and Debugging Tools",
            "Network LAN",
            "SP General Tools and Utilities",
            "RightNowWeb interface",
            "SP C/C++ Compilers",
            "Network WAN",
            "NERSC Training Web Pages",
            "Visualization Services",
            "SP Applications",
            "Training",
            "SP Bug Resolution",
            "PDSF Overall",
            "PDSF Ability to Run Interactively",
            "PDSF Uptime",
            "PDSF Batch Wait Time",
            "PDSF Disk Configuration and I/O Performance",
            "PDSF Queue Structure",
            "PDSF User Environment",
            "PDSF C/C++ Compilers",
            "PDSF General Tools and Utilities",
            "PDSF Libraries",
            "PDSF Performance and Debugging Tools",
            "PDSF Applications",
            "SP Viz Software",
            "PDSF Bug Resolution",
            "PDSF Fortran Compilers",
            "Access Grid classes",
            "NERSC Training Classes",
            "Newton",
            "Escher",
            "Escher Viz Software"
        ],
        [
            "No. of Responses",
            "298",
            "255",
            "245",
            "242",
            "241",
            "233",
            "213",
            "213",
            "210",
            "207",
            "207",
            "200",
            "196",
            "196",
            "192",
            "191",
            "190",
            "186",
            "177",
            "169",
            "162",
            "156",
            "152",
            "137",
            "134",
            "131",
            "127",
            "126",
            "126",
            "126",
            "126",
            "121",
            "117",
            "114",
            "111",
            "109",
            "103",
            "100",
            "98",
            "97",
            "94",
            "94",
            "81",
            "68",
            "64",
            "62",
            "61",
            "59",
            "59",
            "55",
            "54",
            "44",
            "43",
            "42",
            "39",
            "37",
            "34",
            "29",
            "27",
            "24",
            "15",
            "13",
            "8"
        ],
        [
            "Average",
            "6.37",
            "6.13",
            "6.39",
            "6.05",
            "6.23",
            "6.34",
            "6.00",
            "6.04",
            "6.07",
            "6.55",
            "6.12",
            "6.54",
            "5.69",
            "6.36",
            "6.43",
            "6.42",
            "5.24",
            "6.49",
            "5.69",
            "6.24",
            "5.57",
            "6.15",
            "6.34",
            "6.26",
            "6.46",
            "6.27",
            "5.98",
            "6.35",
            "6.46",
            "6.61",
            "6.54",
            "6.07",
            "5.57",
            "6.54",
            "5.98",
            "6.02",
            "6.22",
            "6.12",
            "5.83",
            "4.81",
            "6.00",
            "5.04",
            "5.64",
            "6.41",
            "5.77",
            "6.35",
            "5.93",
            "5.69",
            "6.00",
            "6.33",
            "6.44",
            "5.93",
            "6.00",
            "5.31",
            "5.87",
            "5.08",
            "5.85",
            "6.03",
            "4.67",
            "4.88",
            "5.20",
            "5.23",
            "4.75"
        ],
        [
            "Std. Dev.",
            "0.88",
            "1.06",
            "1.05",
            "1.09",
            "0.94",
            "1.01",
            "1.10",
            "1.20",
            "1.07",
            "0.73",
            "1.10",
            "0.69",
            "1.26",
            "0.84",
            "0.78",
            "0.83",
            "1.52",
            "0.75",
            "1.22",
            "0.90",
            "1.49",
            "1.03",
            "0.97",
            "0.86",
            "0.84",
            "0.86",
            "1.24",
            "1.06",
            "0.88",
            "0.77",
            "0.79",
            "0.99",
            "1.31",
            "0.67",
            "1.04",
            "1.07",
            "1.10",
            "1.02",
            "1.06",
            "1.17",
            "1.04",
            "1.26",
            "1.15",
            "0.87",
            "1.39",
            "1.04",
            "1.12",
            "1.15",
            "0.96",
            "0.77",
            "0.79",
            "1.07",
            "1.07",
            "1.39",
            "1.06",
            "1.46",
            "1.21",
            "1.09",
            "1.41",
            "1.15",
            "1.37",
            "1.30",
            "1.39"
        ],
        [
            "Change from 2002",
            "0.05",
            "0.16",
            "0.01",
            "0.07",
            "0.16",
            "0.04",
            "-0.09",
            "-0.13",
            "-0.03",
            "0.04",
            "0.08",
            "0.08",
            "-0.15",
            "-0.04",
            "0.05",
            "-0.14",
            "-0.17",
            "0.10",
            "-0.23",
            "0.12",
            "0.10",
            "0.18",
            "-0.02",
            "0.05",
            "0.07",
            "0.18",
            "0.03",
            "-0.05",
            "0.11",
            "0.10",
            "0.17",
            "0.10",
            "0.08",
            "NA",
            "0.18",
            "0.08",
            "0.11",
            "NA",
            "-0.06",
            "-0.02",
            "0.30",
            "0.05",
            "0.05",
            "0.15",
            "-0.41",
            "-0.16",
            "0.19",
            "0.06",
            "0.03",
            "-0.05",
            "-0.02",
            "-0.11",
            "-0.24",
            "0.06",
            "-0.34",
            "NA",
            "-0.15",
            "-0.42",
            "NA",
            "-0.25",
            "-0.24",
            "-0.15",
            "NA"
        ],
        [
            "Change from 2001",
            "0.12",
            "0.02",
            "-0.04",
            "0.24",
            "0.20",
            "0.04",
            "-0.18",
            "0.12",
            "0.25",
            "-0.01",
            "0.07",
            "0.08",
            "-0.31",
            "NA",
            "0.61",
            "0.89",
            "0.32",
            "0.12",
            "0.50",
            "0.17",
            "0.86",
            "0.48",
            "0.38",
            "0.32",
            "-0.04",
            "0.27",
            "-0.04",
            "0.12",
            "0.10",
            "-0.02",
            "0.21",
            "0.10",
            "0.88",
            "NA",
            "0.26",
            "NA",
            "0.50",
            "NA",
            "NA",
            "0.30",
            "0.33",
            "0.12",
            "0.19",
            "NA",
            "NA",
            "NA",
            "NA",
            "NA",
            "NA",
            "NA",
            "NA",
            "NA",
            "NA",
            "NA",
            "NA",
            "NA",
            "NA",
            "NA",
            "NA",
            "-0.67",
            "-0.27",
            "0.15",
            "NA"
        ]
    ],
    "pageTitle": "2003 NERSC User Survey Results",
    "title": "",
    "url": "http://www.nersc.gov/news-publications/publications-reports/user-surveys/2003-user-survey-results/?show_all=1",
    "hasHeader": true,
    "headerPosition": "FIRST_ROW",
    "tableType": "RELATION",
    "tableNum": 36,
    "s3Link": "common-crawl/crawl-data/CC-MAIN-2015-32/segments/1438042982013.25/warc/CC-MAIN-20150728002302-00281-ip-10-236-191-2.ec2.internal.warc.gz",
    "recordEndOffset": 605236463,
    "recordOffset": 605150412,
    "tableOrientation": "HORIZONTAL",
    "TableContextTimeStampBeforeTable": "{332=You can see the FY 2003 User Survey text, in which users rated us on a 7-point satisfaction scale. Some areas were also rated on a 3-point importance scale or a 3-point usefulness scale.}",
    "TableContextTimeStampAfterTable": "{100135=I perform all-electron fully relativistic Dirac-Fock calculations for the electronic structure of systems of superheavy elements with hundreds of electrons ( an example is Seaborgium Hexabromide molecular species with 316 electrons!). The formalism we developed in 1975 can handle such problems but the resulting gargantuan calculations require huge amounts of CPU even on the supercomputers. In addition disk storage of ~ 20-50 Gbytes may be needed for each calculation ( which is freed after the completion of the calculation,of course). Since experimental chemical ( and physical) information about the man-made superheavy elements is scarce due to their very short life ( ~ a few seconds at best) and production of ~ half a dozen atoms. Therefore atomic and molecular systems of these superheavy elements are ideal systems to be studied theoretically , and we can make predictions about their chemistry and physics. We have been studying the chemistry ( and physics) of the superheavy elements Rutherfordium ( atomic number Z=104) to the primordial superheavy elements E126 ekaplutonium ( Z=126) and their numerous compounds. The results have been mentioned in the various ERCAP applications and published in open literature and featured in Chemical and Engineering News as News of the Week ( see Dec 16,2002, issue of Chemical and Engineering News). Such calculations can only be performed at the state-of the art superb world-class supercomputing facility like NERSC and nowhere else! This is my motivation for using NERSC facility, which is sine quo non for my theoretical research in the prediction of the Chemistry of man-made superheavy transactinide elements with Z=104-126., 13955=In FY 2003 NERSC implemented initiatives aimed at promoting highly scalable applications as part of the DOE emphasis on large scale computing. For the first time, DOE had in FY 2003 an explicit goal that \"25% of the usage will be accounted for by computations that require at least 1/8 of the total [compute] resource.\" (Note: for FY 2004 this goal is for 50% of the usage, rather than 25%.), 61998=The chief attraction is the sheer size of seaborg. Being able to run on 1024 or more processors allows our group to look at cutting-edge problems., 279478=NERSC response: As of October 2003 NERSC has captured all training sessions using RealPlayer. These are available for replay on the web. On the day of the training classes users can participate in any of the following ways:, 247114=With the old queue structure, we seldom had to wait more than a day for a job to start. With the new queue structure the wait has increased to of order a week. If this continues into FY2004, we will not be able to get our work done. ..., 178947=I believe that they are totally misguided. The emphasis should be on maximizing the SCIENTIFIC output from NERSC. If the best way to do this is for the user to run 100 1-node jobs at a time rather than 1 100-node job, every effort should be made to accommodate him/her. Even for codes which do run well on large numbers of nodes, it makes little sense for a user to use 1/8 or more of the system, unless he/she has an allocation of 1/8 or more of the available CPU time. Even then it might be more useful to run for example 4 jobs using 1/32 of the machine at a time rather than 1 job using 1/8 of the machine. The IBM SP is not designed for fine-grained parallelism requiring communication every 10-1000 floating-point operations. In the final analysis, it should be up to the users to decide how they use their allocations. Most, if not all of us, will choose a usage pattern which maximizes our scientific output. Remember that most of us are in computational science, not in computer science. We are interested in advancing our own fields of research, not in obtaining Gordon Bell awards. ... If it were not for the fact that our FY2003 allocations are nearly exhausted, we would be complaining loudly because with the new Class (Queue) structure which favours \"large\" jobs, we can't get our work done., 8984=NERSC response: Resources were not increased for \"regular long\" types of jobs; rather the priority has been to increase resources for jobs running on more than 32 nodes. This is in line with the DOE Office of Science's goal that 1/4 of all batch resources be applied to jobs that use 1/8 of the available processors. For FY 2004 this goal has been increased to target 1/2 of the batch resources. Perhaps because of this resource prioritization, satisfaction with the SP queue structure dropped by 0.2 points., 7728=NERSC response: In March 2003 limits were extended from 8 to 48 hours for jobs running on 32 or more nodes, and from 8 to 12 hours for jobs run on 31 or fewer nodes. The \"regular long\" class, which provides a 24 hour limit for jobs run on 31 or fewer nodes, was preserved but with restrictions on the number of jobs that can run simultaneously., 58651=Beginning in May, NERSC started participating in Berkeley Lab's Flexible Work Option (FWO) Pilot. FWO means some staff work 9 hours a day and are off one day every two weeks. NERSC always has on duty qualified staff for all areas., 186995=As a startup account, we run jobs on 2000 Cpu s but could not be part of the Reimbursement Project, 183661=It is detrimental to research group like ours. We need NERSC resources to run 100-1000 jobs or so at a time (in serial mode with one processor per job) or 10-20 jobs with 2-3 nodes per job. There are no other resources available to us that would enable us to do this. On the other hand, our jobs are no longer as favored in the queue since they are smaller scale jobs., 11123=NERSC response: All the above are excellent suggestions and we certainly understand the desire for more computational resources. The FY 2004 Seaborg allocation requests were for 2.4 times the amount available to allocate. The reality is that there is no budget for additional hardware acquisitions. Last year we were able to double the number of nodes on Seaborg and this year's rating for available computing hardware increased by 0.2 points., 12407=NERSC response: We have made an effort to keep up-to-date on a wide range of SP topics: IBM compilers, the LoadLeveler batch system, IBM SP specific APIs, and links to IBM redbooks. In addition the presentation of SP information has been streamlined; hopefully information is easier to find now. In August 2003 we received positive comments from ScicomP 8 attendees in regard to how we present IBM documentation., 318564=We are very pleased with the ability to use several 1000 Cpu s ..., 338007=I used computer facilities at the University of Western Ontario in Canada (1990-96), at Auburn University, AL (1997-99), at the University of South Florida, FL (2000), and at the Engineering Research Center at Mississippi State University, MS (2001-present). I think, NERSC is the exemplary organization from which many centers can learn how to operate efficiently, facilitating progress in science., 69728=In years 1997-98 I used Cray T3E to develop lattice quantum theory of scattering processes and performed extensive computations of scattering cross sections. These results, considered today as benchmark, wouldn't be possible without the NERSC support. Since 2001 I have been using IBM SP. My current research concerns parallel discrete-event simulations (PDES) for stochastic modeling of information-driven systems. This research opens a new interdisciplinary area between non-equilibrium statistical physics and computer science. It contributes to basic research in non-equilibrium surface growth and complex systems as well as being a pilot study that applies methods of surface science to performance studies of algorithms for PDES. The access to the NERSC computing facilities allowed me to perform large-scale simulations that wouldn't be possible otherwise. Currently, this study is in the publishing stage. I very much appreciate the opportunity of having access to the NERSC facilities. I hope to use IBM SP and visualization services in my future computational projects., 8249=NERSC response: In March 2003 interactive jobs were given an additional system priority boost (placing them ahead of debug jobs)., 8524=NERSC response: Two new classes to facilitate pre-and-post data processing and data transfers to HPSS were introduced in November, 2003. Jobs run in these classes are charged for one processor's wall clock time., 7407=Every year we institute changes based on the survey. NERSC took a number of actions in response to suggestions from the 2002 user survey., 62379=Large scale computation using 1000 PEs for days, 326483=If you want to encourage big parallel jobs, you might consider giving a discount for jobs over 1024 processors., 62240=One of the very few sites where I can use several 1000 processors., 254360=I commented in the 2002 that interactive access to seaborg was terrible. It still is. YOU NEED TO HAVE DEDICATED _SHARED_ACCESS_ NODES TO SOLVE THIS PROBLEM!!!!! I am sick to death of trying to run Totalview and being DENIED because of \"lack of available resources\" error messages. GET WITH IT ALREADY!!, 107867=I haven't had any problems so far. But I don't understand the need of the exclusive use of ssh protocol 2. I expect to have some problems with connections from both erc.msstate.edu and from my PC at home. I will deal with it in October, when the transition v1-to-v2 occurs.}",
    "textBeforeTable": "You can see the FY 2003 User Survey text, in which users rated us on a 7-point satisfaction scale. Some areas were also rated on a 3-point importance scale or a 3-point usefulness scale. The survey responses provide feedback about every aspect of NERSC's operation, help us judge the quality of our services, give DOE information on how well NERSC is doing, and point us to areas we can improve. The survey results are listed below. Many thanks to the 326 users who responded to this year's User Survey -- this represents the highest response level yet in the six years we have conducted the survey. The respondents represent all five DOE Science Offices and a variety of home institutions: see Respondent Demographics. Response Summary 2003 User Survey Results Show Pagination User Surveys \u00bb 2003 User Survey Results Publications & Reports \u00bb News & Publications \u00bb Home \u00bb \u00a0 \u00a0Twitter \u00a0 Google+",
    "textAfterTable": "DOE Labs 120 36.8 Other Govt Labs 12 3.7 Industry 6 1.8 Private Labs 6 1.8 Organization Number Percent Berkeley Lab 59 18.1 UC Berkeley 17 5.2 Livermore 11 3.4 Oak Ridge 9 2.8 Argonne 8 2.5 Brookhaven 8 2.5 Stanford 8 2.5 U. South Carolina 8 2.5 UC Davis 7 2.1 U. Wisconsin - Madison 7 2.1 Yale 7 2.1 NREL 6 1.8 SLAC 6 1.8 U. Colorado 6 1.8 U. Washington 6 1.8 PNNL 5 1.5 UCLA 5 1.5 Ames Lab 4 1.2 Inst National de Physique 4 1.2 Ohio State 4 1.2 Purdue 4 1.2 U. Texas - Austin 4 1.2 Organization Number Auburn U. 3 Cal Tech 3 Georgia IT 3 Harvard 3 Max Planck Inst. 3",
    "hasKeyColumn": true,
    "keyColumnIndex": 0,
    "headerRowIndex": 0
}