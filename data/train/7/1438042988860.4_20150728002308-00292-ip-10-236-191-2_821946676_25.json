{
    "relation": [
        [
            "Element",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        [
            "Purpose",
            "Root element of an XML grammar",
            "Header declaration of meta content of an HTTP equivalent",
            "Header declaration of XML metadata content",
            "Header declaration of a pronunciation lexicon",
            "Declare a named rule expansion of a grammar",
            "Define a word or other entity that may serve as input",
            "Refer to a rule defined locally or externally",
            "Define an expansion with optional repeating and probability",
            "Define a set of alternative rule expansions",
            "Element contained within a rule definition that provides an example of input that matches the rule",
            "Define an arbitrary string that to be included inline in an expansion which may be used for semantic interpretation"
        ],
        [
            "Section (in [SRGS])",
            "\u00a74.",
            "\u00a74.11.1",
            "\u00a74.11.2",
            "\u00a74.10",
            "\u00a73.",
            "\u00a72.1",
            "\u00a72.2",
            "\u00a72.3",
            "\u00a72.4",
            "\u00a73.3",
            "\u00a72.6"
        ]
    ],
    "pageTitle": "Voice Extensible Markup Language (VoiceXML) Version 2.0",
    "title": "",
    "url": "http://www.w3.org/TR/2002/WD-voicexml20-20020424/",
    "hasHeader": true,
    "headerPosition": "FIRST_ROW",
    "tableType": "RELATION",
    "tableNum": 25,
    "s3Link": "common-crawl/crawl-data/CC-MAIN-2015-32/segments/1438042988860.4/warc/CC-MAIN-20150728002308-00292-ip-10-236-191-2.ec2.internal.warc.gz",
    "recordEndOffset": 822080695,
    "recordOffset": 821946676,
    "tableOrientation": "HORIZONTAL",
    "TableContextTimeStampAfterTable": "{228365=In the case that an input matches more than one active grammar, the list above defines the precedence order. If the input matches more than one active grammar with the same precedence, the precedence is determined using document order. If no grammars are active when an input is expected, the platform must throw an error.semantic event. The error will be thrown in the context of the executing element. Menus behave with regard to grammar activation like their equivalent forms (see Section 2.2)., 231587=It is possible that a grammar will match but not return a semantic interpretation. In this case, the platform will use the raw text string for the utterance as the semantic result. Otherwise, this case is handled exactly as if the semantic interpretation consisted of a simple value., 368667=The length of silence required following user speech before the speech recognizer finalizes a result (either accepting it or throwing a nomatch event). The complete timeout is used when the speech is a complete match of an active grammar.\u00a0 By contrast, the incomplete timeout is used when the speech is an incomplete match to an active grammar., 543115=A platform is not required to implement a grammar that accepts all possible values that might be returned by a builtin. For instance, the currency builtin defines the return value formatting for a very broad range of currencies ([ISO4217]). The platform is not required to support spoken input that includes any of the world's currencies since that can negatively impact recognition accuracy. Similarly, the number builtin can return positive or negative floating point numbers but the grammar is not required to support all possible spoken floating point numbers., 2198=Copyright \u00a92002 W3C\u00ae (MIT, INRIA, Keio), All Rights Reserved. W3C liability, trademark, document use and software licensing rules apply., 240831=This defines two input item variables, 'x' and 'z'. The corresponding slot names are 'x' and 'y' respectively. The next table describes the assignment of these variables depending on which grammar is recognized and what semantic result is returned. The shorthand valueX is used to indicate 'the structured object or simple result value associated with the property x'., 423909=In this last DTMF example, the entry of the last DTMF has brought the grammar to a termination point at which no additional DTMF is allowed by the grammar. Since the termchar is non-empty, the user enters the optional termchar within termtimeout causing the recognized value to be returned (excluding the termchar)., 110261=If a <grammar> element is specified in <choice>, then the external grammar is used instead of an automatically generated grammar. This allows the developer to precisely control the <choice> grammar; for example:, 410810=Leaving the form because the user matched another form, menu, or link\u2019s document-scoped grammar., 224308=A DTMF grammar is distinguished from a speech grammar by the mode attribute on the <grammar> element. An \"xml:lang\" attribute has no effect on DTMF grammar handling. In other respects speech and DTMF grammars are handled identically including the ability to define the grammar inline, by an inline grammar fragment, or by an external grammar reference. The media type handling, scoping and fetching are also identical., 247280=In order to address these potential problems, the committee is looking at various approaches to ensuring consistency between the grammar and the VoiceXML., 475113=This W3C specification is based upon VoiceXML 1.0 submitted by the VoiceXML Forum in May 2000. The VoiceXML Forum authors were: Linda Boyer, IBM; Peter Danielsen, Lucent Technologies; Jim Ferrans, Motorola; Gerald Karam, AT&T; David Ladd, Motorola; Bruce Lucas, IBM; Kenneth Rehor, Lucent Technologies., 410081=The form interpretation algorithm (FIA) drives the interaction between the user and a VoiceXML form or menu. A menu can be viewed as a form containing a single field whose grammar and whose <filled> action are constructed from the <choice> elements., 210465=The following is the equivalent example of the inline grammar defined by the ABNF Form of the W3C Speech Recognition Grammar Specification [SRGS]. Because VoiceXML platforms are not required to support this format it may be less portable., 80078=Each field in this example has a prompt to play in order to elicit a response, a grammar that specifies what to listen for, and an event handler for the help event. The help event is thrown whenever the user asks for assistance. The help event handler catches these events and plays a more detailed prompt., 99923=If a field item is visited, the FIA selects and queues up any prompts based on the field item\u2019s prompt counter and the prompt conditions. Then it activates and listens for the field level grammar(s) and any active higher-level grammars, and waits for a grammar recognition or for some event., 215313=A weight has no effect on DTMF grammars (See Section 3.1.2). Any weight attribute specified in a grammar element whose mode attribute is dtmf is ignored., 220271=If the grammar source does not contain valid content of the selected media type, an error is thrown when the grammar is used., 104058=A menu is a convenient syntactic shorthand for a form containing a single anonymous field that prompts the user to make a choice and transitions to different places based on that choice. Like a regular form, it can have its grammar scoped such that it is active when the user is executing another dialog. The following menu offers the user three choices:, 542287=The builtin types are defined in such a way that a VoiceXML application developer can assume some consistency of user input across implementations. This permits help messages and other prompts to be independent of platform in many instances. For example, the boolean type\u2019s grammar should minimally allow \"yes\" and \"no\" responses in English, but each implementation is free to add other choices, such as \"yeah\" and \"nope\"., 488325=By means of these adapter schema, the VoiceXML schema indirectly references the no-namespace schemas of the grammar and synthesis specifications., 211651=The following is an example of a reference to an external grammar written in the XML Form of the W3C Speech Recognition Grammar Specification [SRGS]., 424234=Figure 18: Timing diagram for termchar non-empty when grammar must terminate., 274118=It is a permissible optimization to begin playing prompts queued during the transitioning state before reaching the waiting state, provided that correct semantics are maintained regarding processing of the input audio received while the prompts are playing, for example with respect to bargein and grammar processing., 209264=The following is an example of inline grammar defined by the XML Form of the W3C Speech Recognition Grammar Specification [SRGS]., 126485=where the first <grammar> references the builtin boolean speech grammar, and the second references the builtin boolean DTMF grammar., 228894=If the form item is modal (i.e., its modal attribute is set to true), all grammars except its own are turned off while waiting for input. If the input matches a grammar in a form or menu other than the current form or menu, control passes to the other form or menu. If the match causes control to leave the current form, all current form data is lost., 205688=The following elements are defined in the XML Form of the W3C Speech Recognition Grammar Specification [SRGS] and are available in VoiceXML 2.0. This document does not redefine these elements. Refer to the W3C Speech Recognition Grammar Specification [SRGS] for definitions and examples., 212876=Weights follow the definition of weights on alternatives in the W3C Speech Recognition Grammar Specification [SRGS \u00a72.4.1]. A weight is a simple positive floating point values without exponentials. Legal formats are \"n\", \"n.\", \".n\" and \"n.n\" where \"n\" is a sequence of one or many digits., 215066=Grammar weights only affect grammar processing. They do not directly affect the post processing of grammar results, including grammar precedence when user input matches multiple active grammar (see Section 3.1.4)., 102676=If an input matches a grammar in this form, then:, 165652=Platform support for recognition of speech grammars during recording is optional. If the platform supports simultaneous recognition and recording, then spoken input matching an active grammar terminates recording. The 'terminating' speech input is accessible via application.lastresult$ and the item's utterance and confidence shadow variables. The audio of the recognized 'terminating' speech input is not available and is not part of the recording., 371430=Several generic properties pertain to DTMF grammar recognition:, 214914=Implicit grammars, such as those in options, do not support weights - use the <grammar> element instead for control over grammar weight., 125758=If both the <grammar> src attribute and an inline grammar are specified, then an error.badfetch is thrown., 211085=An external grammar is specified by an element of the form, 164428=A recording ends when an event is thrown, DTMF or speech input matches an active grammar, or the maxtime interval is exceeded. As an optimization, a platform may end recording after a silence interval (set by the 'finalsilence' attribute) indicating the user has stopped speaking., 422084=Figure 15: Timing diagram for termchar and interdigittimeout, grammar can terminate., 165390=Any DTMF keypress matching an active grammar terminates recording. DTMF keypresses not matching an active grammar are ignored (and therefore do not terminate or otherwise affect recording) and may optionally be removed from the signal by the platform., 291520=The following example show how application.lastresult$ can be used in a field level <catch> to access a <link> grammar recognition result and transition to different dialog states depending on confidence:, 120466=Each input item may have an associated set of shadow variables. Shadow variables are used to return results from the execution of an input item, other than the value stored under the name attribute. For example, it may be useful to know the confidence level that was obtained as a result of a recognized grammar in a <field> element. A shadow variable is referenced as name$.shadowvar where name is the value of the field item\u2019s name attribute, and shadowvar is the name of a specific shadow variable. For example, the <field> element returns a shadow variable confidence. The code fragment below illustrates how this shadow variable is accessed., 212720=Grammar elements, including those in link, field and form elements, can have a weight attribute. The grammar can be inline, external or built-in., 166113=If the termination grammar matched (DTMF or speech) is a local grammar, the recording is placed in the record variable. Otherwise, the record variable is left unfilled (note) and the form interpretation algorithm is invoked. In each case, application.lastresult$ and the item's shadow variables are assigned., 553500=Note that more than one parameter may be specified separated by \";\" as illustrated above. In <grammar> elements, the src attribute URI must start with builtin:grammar/ or builtin:dtmf/ as shown above. When a <grammar> element with the mode set to \"voice\" (the default value) is specified in a <field>, it overrides the default speech grammar implied by the type attribute of the field. Likewise, when a <grammar> element with the mode set to \"dtmf\" is specified in a <field>, it overrides the default DTMF grammar., 3744=This is a W3C Last Call Working Draft for review by W3C Members and other interested parties. Last call means that the working group believes that this specification is ready and therefore wishes this to be the last call for comments. If the feedback is positive, the working group plans to submit it for consideration as a W3C Candidate Recommendation. Comments can be sent until the 24th of May, 2002., 165162=The <record> element contains a 'dtmfterm' attribute as a developer convenience. A 'dtmfterm' attribute with the value 'true' is equivalent to the definition of a local DTMF grammar which matches any DTMF input., 204169=The <grammar> element is used to provide a speech grammar that, 205113=VoiceXML platforms must be a Conforming XML Form Grammar Processor as defined in the W3C Speech Recognition Grammar Specification [SRGS]. While this requires a platform to process documents with one or more \"xml:lang\" attributes defined, it does not require that the platform must be multi-lingual. When an unsupported language is encountered, the platform throws an error.unsupported.language event which specifies the unsupported language in its message variable., 230713=The semantic interpretation returned from a Speech Recognition Grammar Specification [SRGS] grammar must be mapped into one or more VoiceXML ECMAScript variables. The process by which this occurs differs slightly for form- and field-level results; these differences will be explored in the next sections. The format of the semantic interpretation, using either the proposed Natural Language Semantics Markup Language [NLSML] or the ECMAScript-like output format of [SISR], has no impact on this discussion. For the purposes of this discussion, the actual result returned from the recognizer is assumed to have been mapped into an ECMAScript-like format which is identical to the representation in application.lastresult$.interpretation as discussed in Section 5.1.5., 423552=Figure 17: Timing diagram for termchar non-empty and termtimeout when grammar must terminate., 199131=You can also define a link that, when matched, throws an event instead of going to a new document. This event is thrown at the current location in the execution, not at the location where the link is specified. For example, if the user matches this link\u2019s grammar or enters '2' on the keypad, a help event is thrown in the form item the user was visiting and is handled by the best qualified <catch> in the item's scope (see Catch Element Selection for further details):, 551814=Where the <grammar> parameterizes the builtin DTMF grammar, the first <field> parameterizes the builtin DTMF grammar (the speech grammar will be activated as normal) and the second <field> parameterizes both builtin DTMF and speech grammars. Parameters which are undefined for a given grammar type will be ignored; for example, \"builtin:grammar/boolean?y=7\"., 127908=When a simple set of alternatives is all that is needed to specify the legal input values for a field, it may be more convenient to use an option list than a grammar. An option list is represented by a set of <option> elements contained in a <field> element. Each <option> element contains PCDATA that is used to generate a speech grammar. This follows the grammar generation method described for <choice> in Section 2.2. Attributes may be used to specify a DTMF sequence for each option and to control the value assigned to the field's form item variable. When an option is chosen, the value attribute determines the interpretation value for the field's shadow variable and for application.lastresult$., 38168=A link supports mixed initiative. It specifies a grammar that is active whenever the user is in the scope of the link. If user input matches the link\u2019s grammar, control transfers to the link\u2019s destination URI. A link can be used to throw an event or go to a destination URI., 541248=The <field> type attribute in Section 2.3.1 is used to specify a builtin grammar for one of the fundamental types. Platform support for fundamental builtin grammars is optional. If a platform does support builtin types, then it must follow the description given in this appendix as closely as possible. A later version of this specification may provide a mechanism for mapping references to the 'fundamental' builtin grammars to references to application-specific grammars., 428972=The <vxml> element must designate the VoiceXML namespace using the \"xmlns\" attribute [XMLNAMES]. The namespace for VoiceXML is defined to be http://www.w3.org/2001/vxml., 208621=The <grammar> element may be used to specify an inline grammar or an external grammar. An inline grammar is specified by the content of a <grammar> element and defines an entire grammar:, 410545=Grammar activation and deactivation at the form and form item levels., 215668=Appropriate weights are difficult to determine, and guessing weights does not always improve recognition performance. Effective weights are usually obtained by study of real speech and textual data on a paricular platform. Furthermore, a grammar weight is platform specific. Note that different ASR engines may treat the same weight value differently. Therefore, the weight value that works well on particular platform may generate different results on other platforms., 309443=A conforming browser may throw an event that extends a pre-defined event string so long as the event contains the specified pre-defined event string as a dot-separated exact initial substring of its event name. Applications that write catch handlers for the pre-defined events will be interoperable. Applications that write catch handlers for extended event names are not guaranteed interoperability. For example, if in loading a grammar file a syntax error is detected the platform must throw \"error.badfetch\". Throwing \"error.badfetch.grammar.syntax\" is an acceptable implementation., 549406=An example of a <field> element with a builtin grammar type:, 221320=Either an \"src\" attribute or a inline grammar (but not both) must be specified; otherwise, an error.badfetch event is thrown., 245227=3. Mismatches between semantic results and VoiceXML fields: Mapping semantic results to VoiceXML depends on a tight coordination between the ASR grammar and the VoiceXML markup. Since in the current framework there's nothing that enforces consistency between a grammar and the associated VoiceXML dialog, mismatches can occur due to developer oversight. Since the dialog's behaviour during these mismatches is difficult to distinguish from certain normal situations, verifying consistency of information is extremely important. Some examples of mismatches:, 115394=If the accept attribute is \"approximate\", then the choice may be matched when a user says a subphrase of the expression. For example, in response to the prompt \"Stargazer astrophysics news\" a user could say \"Stargazer\", \"astrophysics\", \"Stargazer news\", \"astrophysics news\", and so on. The equivalent grammar may be language and platform dependent., 219728=The media type of the grammar. This value takes precedence over other possible sources of the media type (for instance, the \"Content-type\" field in an HTTP or RTSP exchange, or the file extension). If this is omitted, the interpreter context will attempt to determine the type dynamically (for instance, using server-specified media type, file extension, or content introspection). If the content of the grammar is contained within the element and no media type is specified, the media type is assumed to be an XML grammar., 117574=A menu behaves like a form with a single field that does all the work. The menu prompts become field prompts. The menu event handlers become the field event handlers. The menu grammars become form grammars. As with forms, grammar matches in menu will update the application.lastresult$ array. These variables are described in Section 5.1.5. Generated grammars must always produce simple results whose interpretation and utterance values are identical., 211979=The following example is the equivalent grammar reference for a grammar that is authored using the ABNF Form of the W3C Speech Recognition Grammar Specification [SRGS]., 197038=A <link> element may have one or more grammars which are scoped to the element containing the <link>. Grammar elements contained in the <link> are not permitted to specify scope. When one of these grammars is matched, the link activates, and either:, 421344=The example below shows the situation when a DTMF grammar could terminate, or extend by the addition of more DTMF input, and the user has elected not to provide any further input., 229559=The Speech Recognition Grammar Specification defines a tag element which contains content for semantic interpretation of speech and DTMF grammars (see Section 2.6 of [SRGS])., 79274=The form interpretation algorithm\u2019s first iteration selects the first block, since its (hidden) form item variable is initially undefined. This block outputs the main prompt, and its form item variable is set to true. On the FIA\u2019s second iteration, the first block is skipped because its form item variable is now defined, and the state field is selected because the dialog variable state is undefined. This field prompts the user for the state, and then sets the variable state to the answer. A detailed description of the filling of form item variables from a field-level grammar may be found in Section 3.1.6. The third form iteration prompts and collects the city field. The fourth iteration executes the final block and transitions to a different URI., 422692=Figure 16: Timing diagram for termchar empty when grammar must terminate., 34015=There are two kinds of dialogs: forms and menus. Forms define an interaction that collects values for a set of field item variables. Each field may specify a grammar that defines the allowable inputs for that field. If a form-level grammar is present, it can be used to fill several fields from one utterance. A menu presents the user with a choice of options and then transitions to another dialog based on that choice., 217362=Issue: mode defaults to \"voice\" in [SRGS]. However, if the default were \"voice\" in VoiceXML 2.0 then any referenced DTMF grammar would REQUIRE a 'mode=\"dtmf\"' declaration else the referencing and referenced document would conflict. We solicit reviewer comment on whether a missing mode should have a different interpretation in these specifications., 395368=Note: the VoiceXML DTD includes modified elements from the DTDs of the Speech Recognition Grammar Specification 1.0 [SRGS] and the Speech Synthesis Markup Language 1.0 [SSML]., 88243=Only input items (and not control items) can be filled as a result of matching a form-level grammar. The filling of field variables when using a form-level grammar is described in Section 3.1.6., 100877=If an <initial> is visited, the FIA selects and queues up prompts based on the <initial>\u2019s prompt counter and prompt conditions. Then it listens for the form level grammar(s) and any active higher-level grammars. It waits for a grammar recognition or for an event., 162282=The user is prompted to record a message, and then records it. The recording terminates when one of the following conditions is met: the interval of final silence occurs, a DTMF key is pressed, the maximum recording time is exceeded, or the caller hangs up. The recording is played back, and if the user approves it, is sent on to the server for storage using the HTTP POST method. Notice that like other input items, <record> has grammar, prompt and catch elements. It may also have <filled> actions., 166463=note Although the record variable is not filled with a recording in this case, a match of a non-local grammar may nevertheless result in an assignment of some value to the record variable (see Section 3.1.6)., 213255=A weight is nominally a multiplying factor in the likelihood domain of a speech recognition search. A weight of \"1.0\" is equivalent to providing no weight at all. A weight greater than \"1.0\" positively biases the grammar and a weight less than \"1.0\" negatively biases the grammar. If unspecified, the default weight for any grammar is \"1.0\". If no weight is specified for any grammar element then all grammars are equally likely., 214347=In the example above, the semantics of weights is equivalent to the following XML grammar., 179735=A successful match will terminate the transfer (the connection to the callee); document interpretation continues normally. If no grammars are specified, the platform will not listen to input from the caller. Bargein properties (bargein, bargeintype) apply as normal for prompts queued before and within <transfer>. At the point the outgoing call begins, audio specified by transferaudio begins, and \"hotword\" recognition becomes the only type of bargein supported for speech grammars (until the connection to the far-end is established); therefore the bargeintype property is ignored. The <transfer> element is modal in that no grammar defined outside its scope is active., 233652=The following table illustrates how this result from a form-level grammar would be assigned to various fields within the form. Note that all fields that can be filled in from the interpretation are filled in simultaneously before any fields are visited by the FIA., 422403=In the example below, the entry of the last DTMF has brought the grammar to a termination point at which no additional DTMF is expected. Since termchar is empty, there is no optional terminating character permitted, thus the recognition ends and the recognized value is returned., 204638=The <grammar> element is designed to accommodate any grammar format that meets these two requirements. VoiceXML platforms must support at least one common format, the XML Form of the W3C Speech Recognition Grammar Specification [SRGS]. VoiceXML platforms may support the Augmented BNF (ABNF) Form of the W3C Speech Recognition Grammar Specification [SRGS]. VoiceXML platforms may support other grammar formats., 88471=Also, the form\u2019s grammars can be active when the user is in other dialogs. If a document has two forms on it, say a car rental form and a hotel reservation form, and both forms have grammars that are active for that document, a user could respond to a request for hotel reservation information with information about the car rental, and thus direct the computer to talk about the car rental instead. The user can speak to any active grammar, and have input items set and actions taken in response., 237788=The result from a field-level grammar fills the associated input item in the following manner:, 31479=It must report characters (for example, DTMF) entered by a user. Platforms must support the XML form of DTMF grammars described in the W3C Speech Grammar Recognition Specification [SRGS]. They should also support the Augmented BNF (ABNF) form of DTMF grammars described in the W3C Speech Grammar Recognition Specification [SRGS]., 425929=Figure 21: Timing diagram for incompletetimeout with speech grammar unrecognized., 425692=In the example above, the user provided a utterance that is not as yet recognized by the speech grammar but is the prefix of a legal utterance. After a silence period of incompletetimeout has elapsed, a nomatch event is thrown., 423009=In the example below, the entry of the last DTMF has brought the grammar to a termination point at which no additional DTMF is allowed by the grammar. If the termchar is non-empty, then the user can enter an optional termchar DTMF. If the user fails to enter this optional DTMF within termtimeout, the recognition ends and the recognized value is returned. If the termtimeout is 0s (the default), then the recognized value is returned immediately after the last DTMF allowed by the grammar, without waiting for the optional termchar., 542766=In cases where an application requires specific behavior or different behavior than defined for a builtin, it should use an explicit field grammar. The following are circumstances in which an application must provide an explicit field grammar in order to ensure portability of the application with a consistent user interface:, 212508=A weight for the grammar can be specified by the weight attribute:, 229255=Grammar activation is not affected by the inputmodes property. For instance, if the inputmodes property restricts input to just voice, DTMF grammars will still be activated, but cannot be matched., 220406=The tentative media types for the W3C grammar format are \"application/srgs+xml\" for the XML form and \"application/srgs\" for ABNF grammars., 425185=In the example above, the user provided a utterance that was recognized by the speech grammar. After a silence period of completetimeout has elapsed, the recognized value is returned., 138340=Normal grammar scoping rules apply when visiting an <initial>, as described in Section 3.1.3.. In particular, no grammars scoped to a <field> are active., 432499=A Conforming VoiceXML Processor must be a Conforming Speech Synthesis Markup Language Processor [SSML] and a Conforming XML Grammar Processor [SRGS] except for differences described in this document. If a syntax error is detected processing a grammar document, then an \"error.badfetch\" event must be thrown., 27607=The language accommodates platform diversity in supported audio file formats, speech grammar formats, and URI schemes. While producers of platforms may support various grammar formats the language requires a common grammar format, namely the XML Form of the W3C Speech Recognition Grammar Specification [SRGS], to facilitate interoperability. Similarly, while various audio formats for playback and recording may be supported, the audio formats described in Appendix E must be supported, 31885=It must be able to receive speech recognition grammar data dynamically. It must be able to use speech grammar data in the XML Form of the W3C Speech Recognition Grammar Specification [SRGS]. It should be able to receive speech recognition grammar data in the ABNF form of the W3C Speech Recognition Grammar Specification [SRGS], and may support other formats such as the JSpeech Grammar Format [JSGF] or proprietary formats. Some VoiceXML elements contain speech grammar data; others refer to speech grammar data through a URI. The speech recognizer must be able to accommodate dynamic update of the spoken input for which it is listening through either method of speech grammar data specification., 98476=Note that the FIA may be given an input (a set of grammar slot/slot value pairs) that was collected while the user was in a different form\u2019s FIA. In this case the first iteration of the main loop skips the select and collect phases, and goes right to the process phase with that input., 224752=The following is an example of a simple inline XML DTMF grammar that accepts as input either \"1 2 3\" or \"#\"., 225841=Form grammars are by default given dialog scope, so that they are active only when the user is in the form. If they are given scope document, they are active whenever the user is in the document. If they are given scope document and the document is the application root document, then they are also active whenever the user is in another loaded document in the same application. A grammar in a form may be given document scope either by specifying the scope attribute on the form element or by specifying the scope attribute on the <grammar> element. If both are specified, the grammar assumes the scope specified by the <grammar> element., 92569=The go_ahead field has its modal attribute set to true. This causes all grammars to be disabled except the ones defined in the current form item, so that the only grammar active during this field is the builtin grammar for boolean., 226939=Sometimes a form may need to have some grammars active throughout the document, and other grammars that should be active only when in the form. One reason for doing this is to minimize grammar overlap problems. To do this, each individual <grammar> element can be given its own scope if that scope should be different than the scope of the <form> element itself:, 84446=Note that the grammar alteratives 'amex' and 'american express' return literal values which need to be handled separately in the conditional expressions. Section 3.1.5 describes how semantic attachments in the grammar can be used to return a single representation of these inputs., 421533=Figure 14: Timing diagram for interdigittimeout, grammar is ready to terminate., 223670=The <grammar> element can be used to provide a DTMF grammar that, 117215=This choice would be read from the audio file, or as \"Stargazer Astrophysics News\" if the file could not be played. The grammar for the choice would be the exact phrase \"Stargazer astrophysics news\" gleaned from the PCDATA of the <choice> element\u2019s descendants., 216235=Attributes of <grammar> inherited from the W3C Speech Recognition Grammar Specification [SRGS] are:, 421025=Figure 13: Timing diagram for interdigittimeout, grammar is not ready to terminate., 211506=If the src attribute is defined and there is an inline grammar as content of a grammar element then an error.badfetch event is thrown., 107183=It specifies a speech grammar fragment and/or a DTMF grammar fragment that determines when that choice has been selected., 224132=VoiceXML platforms are required to support the DTMF grammar XML format defined in Appendix D of the [SRGS] to advance application portability., 243659=2. Transitioning semantic results from ASR to VoiceXML: The result of processing the semantic tags of a W3C ASR grammar is the value of the attribute of the root rule when all semantic attachment evaluations have been completed. In addition, the root rule (like all non-terminals) has an associated \"text\" variable which contains the series of tokens in the utterance that is governed by that non-terminal. In the process of making ASR results available to VoiceXML documents, the VoiceXML platform is not only responsible for filling in the VoiceXML fields based on the value of the attribute of the root rule, as described above, but also for filling in the shadow variables of the field. The name$.utterance shadow variable of the field should be the same as the \"text\" variable value for the ASR root rule. The platform is also responsible for instantiating the value of the shadow variable \"name$.confidence\" based on information supplied by the ASR platform, as well as the value of \"name$.inputmode\" based on whether DTMF or speech was processed. Finally, the platform is responsible for making this same information available in the \"application.lastresult$\" variable, defined in Section 5.1.5 (specifically, \"application.lastresult$.utterance\", \"application.lastresult$.inputmode\", and \"application.lastresult$.interpretation\"), with the exception of application.lastresult$.confidence, which the platform sets to the confidence of the entire utterance interpretation., 420840=In Figure 13, the interdigittimeout determines when the nomatch event is thrown because a DTMF grammar is not yet recognized, and the user has failed to enter additional DTMF., 425378=Figure 20: Timing diagram for completetimeout with speech grammar recognized., 551210=The builtin boolean grammar and builtin digits grammar can be parameterized. This is done by explicitly referring to builtin grammars using the \"builtin:\" URI scheme and using a URI-style query syntax of the form type?param=value in the src attribute of a <grammar> element, or in the type attribute of a field, for example:, 370105=The incomplete timeout also applies when the speech prior to the silence is a complete match of an active grammar, but where it is possible to speak further and still match the grammar.\u00a0 By contrast, the complete timeout is used when the speech is a complete match to an active grammar and no further words can be spoken., 488482=For convenience, the VoiceXML schema is reproduced in 0.1, the grammar adapter schema in 0.2 and the synthesis adapter schema in 0.3., 127252=In addition, the \"builtin:\" URI scheme may be used to access platform-specific builtin grammars that are supported by particular interpreter contexts. It is recommended that plaform-specific builtin grammar names begin with the string \"x-\", as this namespace will not be used in future versions of the standard., 221466=When referencing an external grammar, the value of the src attribute is a URI specifying the location of the grammar with an optional fragment for the rulename. Section 2.2 of the Speech Recognition Grammar Specification [SRGS] defines several forms of rule reference. The following are the forms that are permitted on a grammar element in VoiceXML.}",
    "lastModified": "Wed, 24 Apr 2002 22:16:46 GMT",
    "textBeforeTable": "1.4 VoiceXML Elements A link supports mixed initiative. It specifies a grammar that is active whenever the user is in the scope of the link. If user input matches the link\u2019s grammar, control transfers to the link\u2019s destination URI. A link can be used to throw an event or go to a destination URI. 1.3.6 Links Events are thrown by the platform under a variety of circumstances, such as when the user does not respond, doesn't respond intelligibly, requests help, etc. The interpreter also throws events if it finds a semantic error in a VoiceXML document. Events are caught by catch elements or their syntactic shorthand. Each element in which an event can occur may specify catch elements. Catch elements are also inherited from enclosing elements \"as if by copy\". In this way, common event handling behavior can be specified at any level, and it applies to all lower levels. VoiceXML provides a form-filling mechanism for handling \"normal\" user input. In addition, VoiceXML defines a mechanism for handling events not covered by the form mechanism. 1.3.5 Events Each dialog has one or more speech and/or DTMF grammars associated with it. In machine directed applications, each dialog\u2019s grammars are active only when the user is in that dialog. In mixed initiative applications, where the user and the machine alternate in determining what to do next, some of the dialogs are flagged to make their grammars active (i.e., listened for) even",
    "textAfterTable": "<exit> Exit a session 5.3.9 <field> Declares an input field in a form 2.3.1 <filled> An action executed when fields are filled 2.4 <form> A dialog for presenting information and collecting data 2.1 <goto> Go to another dialog in the same or different document 5.3.7 <grammar> Specify a speech recognition or DTMF grammar 3.1 <help> Catch a help event 5.2.3 <if> Simple conditional logic 5.3.4 <initial> Declares initial logic upon entry into a (mixed-initiative) form 2.3.3 <link> Specify a transition common to all dialogs in the link\u2019s scope 2.5 <log> Generate a debug message 5.3.13 <menu> A dialog for choosing amongst alternative destinations 2.2 <meta> Define a metadata item as a name/value pair 6.2.1 <metadata> Define metadata information using a metadata schema 6.2.2 <noinput> Catch a noinput event 5.2.3 <nomatch> Catch a nomatch event 5.2.3 <object> Interact with a custom extension 2.3.5 <option> Specify an option in a <field> 2.3 <param> Parameter in <object> or <subdialog> 6.4 <prompt> Queue speech synthesis and audio output to the user 4.1 <property> Control implementation platform settings. 6.3 <record> Record an",
    "hasKeyColumn": false,
    "keyColumnIndex": -1,
    "headerRowIndex": 0
}