{
    "relation": [
        [
            "MPI Tasks per NUMA Region",
            "1",
            "2",
            "4"
        ],
        [
            "Threads per MPI task",
            "8",
            "4",
            "2"
        ],
        [
            "aprun syntax",
            "aprun -n \u2026 -N 4 -S 1 -d 8 \u2026",
            "aprun -n \u2026 -N 8 -S 2 -d 4 \u2026",
            "aprun -n \u2026 -N 16 -S 4 -d 2 \u2026"
        ]
    ],
    "pageTitle": "Best Practice Guide - Cray XE/XC - PRACE Research Infrastructure",
    "title": "",
    "url": "http://www.prace-ri.eu/best-practice-guide-cray-xe-xc-html/",
    "hasHeader": true,
    "headerPosition": "FIRST_ROW",
    "tableType": "RELATION",
    "tableNum": 25,
    "s3Link": "common-crawl/crawl-data/CC-MAIN-2015-32/segments/1438042988311.72/warc/CC-MAIN-20150728002308-00339-ip-10-236-191-2.ec2.internal.warc.gz",
    "recordEndOffset": 657926011,
    "recordOffset": 657856682,
    "tableOrientation": "HORIZONTAL",
    "TableContextTimeStampBeforeTable": "{6314=Tier-1 \u2013 Sisu, CSC, Finland Sisu is a Cray XC30 system installed at CSC\u2019s Kajaani site. The first stage has been completed, providing 250 TFlops theoretical peak performance. The second stage will be completed in 2014, to bring Sisu into the petaflops class., 5133=Tier-1 \u2013 Archer, EPCC, UK Archer will be the UK\u2019s front-line national supercomputing service, replacing HECToR in 2014, and is provided by the Archer Partners including EPCC. The Archer service consists of a Cray XC30 supercomputer with a peak performance of greater than 1.5 PFlops, a high-performance parallel file system (Lustre), and an archive facility., 10397=Tier-0 \u2013 HERMIT, HLRS, Germany Hermit is a PRACE Tier-0 system located at HLRS, Germany. This system will be installed in 3 steps and currently the system is in Installation Step 1; the final step: Installation Step 2 is planned for 2014 when a fully integrated phase 1 system will be available., 29838=11 December 2013, 7853=Tier-1 \u2013 Lindgren, KTH, Sweden Lindgren is a Cray XE6 system, based on the AMD Opteron 12-core \u201cMagny-Cours\u201d (2.1 GHz) processors and the Cray Gemini interconnect technology. It is located at PDC Center for High Performance Computing. It has 16 racks with a theoretical peak performance of 305 TFlops. Lindgren was ranked in place 31 amongst the 500 most powerful computer systems in the world (Top500, June 2011). Lindgren is named after the Swedish 20th Century children\u2019s book author Astrid Lindgren.}",
    "TableContextTimeStampAfterTable": "{44325=The amount of memory per node varies with the system, and Archer has some nodes with larger amounts of memory (128 GB per node). On Sisu, all the processors on a compute node share 32 GB of 1600 MHz DDR3 memory. On Archer, all the processors on a compute node share 64/128 GB of 1866 MHz DDR3 memory., 32099=These instructions can use the floating-point unit (FPU) to operate on multiple floating point numbers simultaneously as long as the numbers are contiguous in memory. SSE instructions contain a number of different operations (for example: arithmetic, comparison, type conversion) that operate on two operands in 128-bit registers. AVX instructions expand SSE to allow operations to operate on three operands and on a data path expanded from 128- to 256-bits. In the E5-2600 architecture each core has a 256-bit floating point pipeline., 111326=This PBSPro job submission script runs 16, 2048-core CP2K simulations inparallel with the input in the directories \u2019simulation1\u2019, \u2019simulation2\u2019, etc.:, 43845=Note 1: 7 groups on Archer have 2632 nodes with 64 GB memory per node, 1 group has 384 nodes with 128 GB memory per node., 152715=this will produce a text report (in castep_samp_1024.pat) listing the various routines in the programand how much time was spent in each one during the run (see below for a discussion on the contentsof this file). It also produces a castep+samp+25370-14s.ap2 file and a castep+samp+25370-14s.apa file:, 28512=Cray XC compute nodes contain two Intel Xeon E5-2600 series processors. Depending on the site these can be either 8-core E5-2670 (Sandy Bridge) or 12-core E5-2697 (Ivy Bridge), and each core can support 2 Hyper-Threads. These individual processors are connected to each other by two QuickPath Interconnect (QPI) links. The memory in a node is shared between the two processors, with non-uniform memory access: each processor consists of one NUMA region (containing either 8 or 12 cores) and access to the processor\u2019s own memory region is faster than access to the other processor\u2019s memory region., 164106=In the original report (castep_samp_1024.pat) we also set a table reporting on the wall clocktime of the program. The table reports the minimum, maximum and median values of wall clocktime from the full set of parallel tasks. The table also lists the maximum memory usage ofeach of the parallel tasks:, 152114=The actual name is dependent on the name of your instrumented executable and the process ID. In thiscase we used 1024 cores so we have got a directory., 34794=The E5-2697 processor has a similar structure to the E5-2670, except with 12 cores and with a corresponding increase in L3 cache to 30 MB, still with fast access from each core to its \u2018local\u2019 2.5 MB of the L3 cache., 212570=Pure MPI job using 1024 MPI tasks (-n option) with 16 tasks per node (-N option):, 33872=Figure 2.3: Overview of an E5-2670 processor. Each core is directly connected to 2.5 MB of the L3 cache, and all the components are connected with a bidirectional ring interconnection. There is an integrated memory controller connected to DDR3 memory, dual QPI links and a 40-lane PCIe3 link. Image courtesy of Christopher Dahnken, Intel; taken from his talk \u2018Intel Sandy Bridge Overview: Understanding the Core\u2019 given during the \u2018Get \u201cup to speed\u201d with the Cray Cascade\u2019 tutorial hosted by CSCS, 11-14 March 2013., 32977=Each E5-2670 processor consists of one NUMA region and contains 8 cores. Each core contains one 256-bit Floating Point Unit (FPU), executing one 256-bit AVX vector instruction or one 128-bit SSE instruction per cycle. See Figure 2.3 for an overview of the E5-2670 architecture., 109148=The total number of cores requested for a job of this type is the sum of the number of cores required forall the simulations in the script. For example, if we have 16 simulations which each run using 2048 coresthen we would need to ask for 32768 cores (1024 nodes on a 32-core per node system)., 109758=The differences from specifying a single aprun command to specifying multiple \u2019aprun\u2019 commands in yourjob submission script is that each of the aprun command must be run in the background (i.e., appendedwith an &) and there must be a \u2019wait\u2019 command after the final aprun command. For example, to run 4CP2K simulations which each use 2048 cores (8192 cores in total) and 32 cores per node:, 211544=Pure MPI job using 1024 MPI tasks (-n option) with 32 tasks per node (-N option):}",
    "lastModified": "Fri, 31 Jul 2015 19:21:13 GMT",
    "textBeforeTable": "Optimal MPI/OpenMP task/thread affinities for 32-core Cray XE compute nodes We always recommend that OpenMP code does not span multiple NUMAregions on Cray XE/XC systems. In general, it has been found that itis very difficult to gain any parallel performance when using OpenMPparallel regions that span multiple NUMA regions on a Cray XE computenode \u2013 it is expected that this will be true for Cray XC compute nodesalso. For this reason, you will generally find that it is best to useone of the following task/thread layouts if your code combines MPI andOpenMP. You can overcome these limitations, when using the GNU compiler suite,by initialising your data in parallel (within a parallel region).This can give improved performance if all the subsequent data accessis only from each thread to its own NUMA region, but poorerperformance if threads access data from other NUMA regions. On Cray XE/XC systems, when using the GNU compiler suite, the locationof the thread that initialises the data can determine the location ofthe data. This means that if you initialise your data in the serialportion of the code then the location of the data will be on the NUMAregion associated with thread 0. This behaviour can have implicationsfor performance in the parallel regions of the code if a thread from adifferent NUMA region then tries to access that data. If you areusing the Cray or PGI compiler suites then there is no guarantee ofwhere shared data will be located if your OpenMP code spans multipleNUMA regions.",
    "textAfterTable": "Optimal MPI/OpenMP task/thread affinities for 24-core Cray XE compute nodes MPI Tasks per NUMA Region Threads per MPI task aprun syntax 1 6 aprun -n \u2026 -N 4 -S 1 -d 6 \u2026 2 3 aprun -n \u2026 -N 8 -S 2 -d 3 \u2026 3 2 aprun -n \u2026 -N 12 -S 3 -d 2 \u2026 Optimal MPI/OpenMP task/thread affinities for 16-core Cray XC compute nodes MPI Tasks per NUMA Region Threads per MPI task aprun syntax 1 8 aprun -n \u2026 -N 2 -S 1 -d 8 \u2026 2 4 aprun -n \u2026 -N 4 -S 2 -d 4 \u2026 4 2 aprun -n \u2026 -N 8 -S 4 -d 2 \u2026 Optimal MPI/OpenMP task/thread affinities for 24-core Cray XC compute nodes MPI Tasks per NUMA Region Threads per MPI task aprun syntax",
    "hasKeyColumn": false,
    "keyColumnIndex": -1,
    "headerRowIndex": 0
}