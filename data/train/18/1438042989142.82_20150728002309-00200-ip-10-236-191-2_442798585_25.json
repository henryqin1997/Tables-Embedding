{
    "relation": [
        [
            "Patent application number",
            "20120249443",
            "20130212501",
            "20130271491",
            "20130278499",
            "20130290876",
            "20130293584",
            "20130307875",
            "20140007147",
            "20140055361",
            "20140104206",
            "20150131913"
        ],
        [
            "Description",
            "VIRTUAL LINKS BETWEEN DIFFERENT DISPLAYS TO PRESENT A SINGLE VIRTUAL OBJECT - Virtual links are used between two different displays to represent a single virtual object. In one example, the invention includes generating a three-dimensional space having a first display of a real first device and a second display of a real second device and a space between the first display and the second display, receiving a launch command as a gesture with respect to the first display, the launch command indicating that a virtual object is to be launched through the space toward the second display, determining a trajectory through the space toward the second display based on the received launch command, and presenting a portion of the trajectory on the second display.",
            "PERCEPTUAL COMPUTING WITH CONVERSATIONAL AGENT - Perceptual computing with a conversational agent is described. In one example, a method includes receiving a statement from a user, observing user behavior, determining a user context based on the behavior, processing the user statement and user context to generate a reply to the user, and presenting the reply to the user on a user interface.",
            "LOCAL SENSOR AUGMENTATION OF STORED CONTENT AND AR COMMUNICATION - The augmentation of stored content with local sensors and AR communication is described. In one example, the method includes gathering data from local sensors of a local device regarding a location, receiving an archival image at the local device from a remote image store, augmenting the archival image using the gathered data, and displaying the augmented archival image on the local device.",
            "GESTURE INPUT WITH MULTIPLE VIEWS, DISPLAYS AND PHYSICS - Gesture input with multiple displays, views, and physics is described. In one example, a method includes generating a three dimensional space having a plurality of objects in different positions relative to a user and a virtual object to be manipulated by the user, presenting, on a display, a displayed area having at least a portion of the plurality of different objects, detecting an air gesture of the user against the virtual object, the virtual object being outside the displayed area, generating a trajectory of the virtual object in the three-dimensional space based on the air gesture, the trajectory including interactions with objects of the plurality of objects in the three-dimensional space, and presenting a portion of the generated trajectory on the displayed area.",
            "AUGMENTED REALITY REPRESENTATIONS ACROSS MULTIPLE DEVICES - Methods and apparatus to produce augmented reality representations across multiple devices are described. In one example, operation include generating a virtual object, generating a reality space including a first display, and presenting the virtual object in the reality space including the first display on a second display. Further operations include tracking a location of the virtual object in the reality space as the virtual object moves through the reality space, updating the presentation of the virtual object on the second display using the tracked location, and presenting the virtual object on the first display when the tracked location of the virtual object coincides with the location of the first display in the reality space.",
            "USER-TO-USER COMMUNICATION ENHANCEMENT WITH AUGMENTED REALITY - The enhancement of user-to-user communication with augmented reality is described. In one example, a method includes receiving virtual object data at a local device from a remote user, generating a virtual object using the received virtual object data, receiving an image at the local device from a remote image store, augmenting the received image at the local device by adding the generated virtual object to the received image, and displaying the augmented received image on the local device.",
            "AUGMENTED REALITY CREATION USING A REAL SCENE - The creation of augmented reality is described using a real scene. In one example, a process includes observing a real scene through a camera of a device, observing a user gesture through the camera of the device, presenting the scene and the gesture on the display of the device, generating a virtual object and placing it in the scene based on the observed user gesture, and presenting the virtual object in the real scene on the display.",
            "PERFORMANCE ANALYSIS FOR COMBINING REMOTE AUDIENCE RESPONSES",
            "INTERACTIVE DRAWING RECOGNITION - Interactive drawing recognition is described. In one example, a command is received indicating a type of drawing and a user drawing is observed. A library of drawing templates associated with the drawing type is accessed based on the command. The observed drawing is compared to the drawing templates to identify the observed drawing, and attributes are assigned to the identified drawing.",
            "CREATION OF THREE-DIMENSIONAL GRAPHICS USING GESTURES - Three-dimensional virtual objects are created using gestures. In one example, a selection of a shape is received. The selected shape is presented on a display. A gesture is observed to move at least a part of the presented shape away from the display and the presented shape is modified based on the observed gesture in the direction away from the display corresponding to the observed gesture. The modified shape is presented as a three dimensional virtual object after modifying the presented shape based on the observed gesture.",
            "INTERACTIVE DRAWING RECOGNITION USING STATUS DETERMINATION - Interactive drawing recognition is described using status determination. In one example, a computer system status is determined and then used to determine a type of drawing. A user drawing is observed and a library of drawing templates associated with the determined drawing type is accessed. The observed drawing is compared to the drawing templates to identify the observed drawing, and attributes are assigned to the identified drawing."
        ],
        [
            "Published",
            "10-04-2012",
            "08-15-2013",
            "10-17-2013",
            "10-24-2013",
            "10-31-2013",
            "11-07-2013",
            "11-21-2013",
            "01-02-2014",
            "02-27-2014",
            "04-17-2014",
            "05-14-2015"
        ]
    ],
    "pageTitle": "Anderson, OR - Patent applications",
    "title": "",
    "url": "http://www.faqs.org/patents/inventor/anderson-or-3/",
    "hasHeader": true,
    "headerPosition": "FIRST_ROW",
    "tableType": "RELATION",
    "tableNum": 25,
    "s3Link": "common-crawl/crawl-data/CC-MAIN-2015-32/segments/1438042989142.82/warc/CC-MAIN-20150728002309-00200-ip-10-236-191-2.ec2.internal.warc.gz",
    "recordEndOffset": 442832847,
    "recordOffset": 442798585,
    "tableOrientation": "HORIZONTAL",
    "textBeforeTable": "Glen J. Anderson, Portland, OR US //]]> BarDraw('3857303',['Patent applications by Glen Anderson, Beaverton, OR US'],[[1,4,1]],['2011','2014','2015'],0); // <script type=\"text/javascript\"> </div> <div id=\"legend3857303\" align=\"center\"></div> </div> <canvas id=\"bg3857303\" style=\"max-width: 500px; width: 100%;\"></canvas> <div> <p align=\"center\">Patent applications by Glen Anderson, Beaverton, OR US</p> <div class=\"bar3857303\" style=\"max-width: 500px; width: 100%; float:left; \"> <div align=\"left\" style=\"padding-top:20px;\"> </table> </tbody> </tr> <td>01-08-2015</td> <td><a href=\"/patents/app/20150009364\">MANAGEMENT AND ACCESS OF MEDIA WITH MEDIA CAPTURE DEVICE OPERATOR PERCEPTION DATA</a> - Operator-centric perception data-driven systems and techniques for managing and accessing personal media captured by a device while under the control of the operator. Operator perception data, for example, parameterizing a physiological state, attribute, or mood of the operator during a media capture event originating certain media data, is associated, as an additional input at media capture time, with subject-centric media data generated by the capture device. This association may then be subsequently stored to enrich the media data, and automate, or otherwise simplify post-capture access and processing of the captured media data.</td> <td>20150009364</td> <tr> </tr> <td>12-11-2014</td> <td><a href=\"/patents/app/20140364097\">DYNAMIC VISUAL PROFILES</a> - Systems and methods may provide for identifying one or more phrases in a conversation between a first user and a second user. In addition, a first visual profile may be generated based on the one or more phrases and a real-time recommendation may be generated based on the first visual profile. In one example, generating the first visual profile includes using the one or more phrases to obtain one or more images and incorporating the one",
    "textAfterTable": "Patent applications by Glen J. Anderson, Portland, OR US Glen J. Anderson, Beaverton, OR US Patent application number Description Published 20110161890 Using multi-modal input to control multiple objects on a display - Embodiments of the invention are generally directed to systems, methods, and machine-readable mediums for implementing gesture-based signature authentication. In one embodiment, a system may include several modal input devices. Each modal input device is capable of retrieving a stream of modal input data from a user. The system also includes modal interpretation logic that can interpret each of the retrieved modal input data streams into a corresponding of set of actions. The system additionally includes modal pairing logic to assign each corresponding set of actions to control one of the displayed objects. Furthermore, the system has modal control logic which causes each displayed object to be controlled by its assigned set of actions. 06-30-2011 20110205148 Facial Tracking Electronic Reader - Facial actuations, such as eye actuations, may be used to detect user inputs to control the display of text. For example, in connection with an electronic book reader, facial actuations and, particularly, eye actuations, can be interpreted to indicate when the turn a page,",
    "hasKeyColumn": false,
    "keyColumnIndex": -1,
    "headerRowIndex": 0
}