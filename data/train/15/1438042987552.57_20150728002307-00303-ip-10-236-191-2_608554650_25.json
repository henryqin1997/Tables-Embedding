{
    "relation": [
        [
            "Importance",
            "Very Important",
            "Somewhat Important"
        ],
        [
            "Average Score",
            "2.50 - 3.00",
            "1.50 - 2.49"
        ]
    ],
    "pageTitle": "2008/2009 NERSC User Survey Results",
    "title": "",
    "url": "http://www.nersc.gov/news-publications/publications-reports/user-surveys/2008-2009-user-survey-results/?show_all=1",
    "hasHeader": true,
    "headerPosition": "FIRST_ROW",
    "tableType": "RELATION",
    "tableNum": 25,
    "s3Link": "common-crawl/crawl-data/CC-MAIN-2015-32/segments/1438042987552.57/warc/CC-MAIN-20150728002307-00303-ip-10-236-191-2.ec2.internal.warc.gz",
    "recordEndOffset": 608610791,
    "recordOffset": 608554650,
    "tableOrientation": "HORIZONTAL",
    "TableContextTimeStampBeforeTable": "{321=You can see the 2008/2009 User Survey text, in which users rated us on a 7-point satisfaction scale. Some areas were also rated on a 3-point importance scale or a 3-point usefulness scale.}",
    "TableContextTimeStampAfterTable": "{42016=On the 2009 survey Franklin's Disk configuration and I/O performance received an average score of 5.60, a statistically significant increase over the previous year by 0.46 points., 43543=NERSC response: In 2008 NERSC improved the interactive PDSF nodes to more powerful, larger memory nodes. In early 2009, we re-organized the user file systems on PDSF to allow for failover, reducing the impact of hardware failures on the system. We also upgraded the network connectivity to the files ystem server nodes to allow for greater bandwidth. In addition, NERSC added a queue to allow for short debug jobs., 306932=... The login nodes are very underpowered, I had issues in April with two htars overloading the node. I have often found myself waiting for an 'ls' to complete. I put htars into batch scripts because they will exceed the interactive time limit., 42366=NERSC response: In the past year NERSC and Cray assembled a team of about 20 people to thoroughly analyze system component layouts, cross interactions and settings; to review and analyze past causes of failures; and to propose and test software and hardware changes. Intense stabilization efforts took place between March and May, with improvements implemented throughout April and May., 40688=Every year we institute changes based on the previous year survey. In 2008 and early 2009 NERSC took a number of actions in response to suggestions from the 2007/2008 user survey., 284062=NERSC has very reliable hardware, excellent administration, and a high throughput. Consultants there have helped me very much with projects and problems and responded with thoughtful messages for me and my problem, as opposed to terse or cryptic pointers to information elsewhere. The HPSS staff helped me set up one of the earliest data sharing archives in 1998, now part of a larger national effort toward Science Gateways. (see: http://www.lbl.gov/cs/Archive/news052609b.html) This archive has a venerable place in the lattice community and is known throughout the community as \"The NERSC Archive\". In fact until recently, the lingua franca for exchanging lattice QCD data was \"NERSC format\", a protocol developed for the archive at NERSC., 290682=NERSC provides accessible large-scale (>2000 core) machines., 42791=As a result of these efforts, Franklin's overall availability went from an average of 87.6 percent in the six months prior to April to an average of 94.97 percent in the April through July 2009 period. In the same period, Mean Time Between Interrupts improved from an average of 1 day 22 hours h39 minutes to 3 days 20 hours 36 minutes., 254627=Consulting support asked me to recompile my software. I asked for help, and my request was ignored. I later received an email to ask if I was able to get my software recompiled, as CS wanted to get some performance data. I replied, yes, I recompiled my software, but it was due to me spending much of my time trying to find where certain libraries were located that CS would have known quite easily. This software (AMBER) was the most recent version (10 - available April 2009), yet NERSC still does not offer that version., 19119=The MPP respondents were classified as \"large\" (if their usage was over 250,000 hours), \"medium\" (usage between 10,000 and 250,000 hours) and \"small\". Satisfaction differences between these three groups are shown in the table below. Comparing their scores with the scores of all the 2007/2008 respondents, this year's smaller users were the most satisfied, and the larger users the least satisfied., 4284=Highlights of the 2009 user survey responses include:, 43996=On the 2009 survey the PDSF \"Ability to run interactively\" score increased significantly by 0.60 points and moved into the \"mostly satisfied - high\" range. The PDSF \"Disk configuration and I/O performance\" score increased by 0.41 points, but this increase was not statistically significant (at the 90 percent confidence level)., 306000=... Better MPP computational reliability. Although it has improved since the worst levels earlier this year, I still regularly have jobs fail periodically for unknown and irreproducible reasons. I write restart files very frequently, particularly for large size jobs, which probably not very efficient even with parallel io. This is roughly 4x more than in seaborg/bassi days (every 500 times steps versus every 2000), while 6-8 hr wallclock jobs now run 4000-6000 time steps, at higher resolution instead of 2000-4000., 43149=The Franklin uptime score in the 2009 survey (which opened in May) did not reflect these improvements. NERSC anticipates an improved score on next year's survey.}",
    "textBeforeTable": "You can see the 2008/2009 User Survey text, in which users rated us on a 7-point satisfaction scale. Some areas were also rated on a 3-point importance scale or a 3-point usefulness scale. The survey responses provide feedback about every aspect of NERSC's operation, help us judge the quality of our services, give DOE information on how well NERSC is doing, and point us to areas we can improve. The survey results are listed below. The respondents represent all six DOE Science Offices and a variety of home institutions: see Respondent Demographics. The PDSF hours used by the PDSF survey respondents represents 36.8 percent of total NERSC PDSF usage as of the end of the survey period. The MPP hours used by the survey respondents represents 70.2 percent of total NERSC MPP usage as of the end of the survey period. The overall response rate for the 3,134 authorized users during the survey period was 13.4%. 36.6 percent of users who had used between 10,000 and 250,000 XT4-based hours responded 77.4 percent of users who had used more than 250,000 XT4-based hours when the survey opened responded Many thanks to the 421 users who responded to this year's User Survey. The response rate is comparable to last year's and both are significantly increased from previous years: Response Survey",
    "textAfterTable": "6,219 5 Somewhat Satisfied 1,488 4 Neutral 1,032 3 Somewhat Dissatisfied 366 2 Mostly Dissatisfied 100 1 Very Dissatisfied 88 Importance Score Meaning 3 Very Important 2 Somewhat Important 1 Not Important Usefulness Score Meaning 3 Very Useful 2 Somewhat Useful 1 Not at All Useful The average satisfaction scores from this year's survey ranged from a high of 6.68 (very satisfied) to a low of 4.71 (somewhat satisfied). Across 94 questions, users chose the Very Satisfied rating 8,060 times, and the Very Dissatisfied rating 90 times. The scores for all questions averaged 6.15, and the average score for overall satisfaction with NERSC was 6.21. See All Satisfaction Ratings. For questions that spanned previous surveys, the change in rating was tested for significance (using the t test at the 90% confidence level). Significant increases in satisfaction are shown in blue; significant decreases in satisfaction are shown in red. Significance of Change significant",
    "hasKeyColumn": true,
    "keyColumnIndex": 0,
    "headerRowIndex": 0
}