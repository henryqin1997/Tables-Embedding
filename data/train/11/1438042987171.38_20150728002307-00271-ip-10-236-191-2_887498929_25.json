{
    "relation": [
        [
            "Name",
            "channel",
            "type",
            "hdfs.path",
            "hdfs.filePrefix",
            "hdfs.fileSuffix",
            "hdfs.inUsePrefix",
            "hdfs.inUseSuffix",
            "hdfs.rollInterval",
            "hdfs.rollSize",
            "hdfs.rollCount",
            "hdfs.idleTimeout",
            "hdfs.batchSize",
            "hdfs.codeC",
            "hdfs.fileType",
            "hdfs.maxOpenFiles",
            "hdfs.minBlockReplicas",
            "hdfs.writeFormat",
            "hdfs.callTimeout",
            "hdfs.threadsPoolSize",
            "hdfs.rollTimerPoolSize",
            "hdfs.kerberosPrincipal",
            "hdfs.kerberosKeytab",
            "hdfs.proxyUser",
            "hdfs.round",
            "hdfs.roundValue",
            "hdfs.roundUnit",
            "hdfs.timeZone",
            "hdfs.useLocalTimeStamp",
            "hdfs.closeTries",
            "hdfs.retryInterval",
            "serializer",
            "serializer.*"
        ],
        [
            "Default",
            "\u2013",
            "\u2013",
            "\u2013",
            "FlumeData",
            "\u2013",
            "\u2013",
            ".tmp",
            "30",
            "1024",
            "10",
            "0",
            "100",
            "\u2013",
            "SequenceFile",
            "5000",
            "\u2013",
            "\u2013",
            "10000",
            "10",
            "1",
            "\u2013",
            "\u2013",
            "",
            "false",
            "1",
            "second",
            "Local Time",
            "false",
            "0",
            "180",
            "TEXT",
            ""
        ],
        [
            "Description",
            "",
            "The component type name, needs to be hdfs",
            "HDFS directory path (eg hdfs://namenode/flume/webdata/)",
            "Name prefixed to files created by Flume in hdfs directory",
            "Suffix to append to file (eg .avro - NOTE: period is not automatically added)",
            "Prefix that is used for temporal files that flume actively writes into",
            "Suffix that is used for temporal files that flume actively writes into",
            "Number of seconds to wait before rolling current file (0 = never roll based on time interval)",
            "File size to trigger roll, in bytes (0: never roll based on file size)",
            "Number of events written to file before it rolled (0 = never roll based on number of events)",
            "Timeout after which inactive files get closed (0 = disable automatic closing of idle files)",
            "number of events written to file before it is flushed to HDFS",
            "Compression codec. one of following : gzip, bzip2, lzo, lzop, snappy",
            "File format: currently SequenceFile, DataStream or CompressedStream (1)DataStream will not compress output file and please don\u2019t set codeC (2)CompressedStream requires set hdfs.codeC with an available codeC",
            "Allow only this number of open files. If this number is exceeded, the oldest file is closed.",
            "Specify minimum number of replicas per HDFS block. If not specified, it comes from the default Hadoop config in the classpath.",
            "Format for sequence file records. One of \u201cText\u201d or \u201cWritable\u201d (the default).",
            "Number of milliseconds allowed for HDFS operations, such as open, write, flush, close. This number should be increased if many HDFS timeout operations are occurring.",
            "Number of threads per HDFS sink for HDFS IO ops (open, write, etc.)",
            "Number of threads per HDFS sink for scheduling timed file rolling",
            "Kerberos user principal for accessing secure HDFS",
            "Kerberos keytab for accessing secure HDFS",
            "",
            "Should the timestamp be rounded down (if true, affects all time based escape sequences except %t)",
            "Rounded down to the highest multiple of this (in the unit configured using hdfs.roundUnit), less than current time.",
            "The unit of the round down value - second, minute or hour.",
            "Name of the timezone that should be used for resolving the directory path, e.g. America/Los_Angeles.",
            "Use the local time (instead of the timestamp from the event header) while replacing the escape sequences.",
            "Number of times the sink must try renaming a file, after initiating a close attempt. If set to 1, this sink will not re-try a failed rename (due to, for example, NameNode or DataNode failure), and may leave the file in an open state with a .tmp extension. If set to 0, the sink will try to rename the file until the file is eventually renamed (there is no limit on the number of times it would try). The file may still remain open if the close call fails but the data will be intact and in this case, the file will be closed only after a Flume restart.",
            "Time in seconds between consecutive attempts to close a file. Each close call costs multiple RPC round-trips to the Namenode, so setting this too low can cause a lot of load on the name node. If set to 0 or less, the sink will not attempt to close the file if the first attempt fails, and may leave the file open or with a \u201d.tmp\u201d extension.",
            "Other possible options include avro_event or the fully-qualified class name of an implementation of the EventSerializer.Builder interface.",
            ""
        ]
    ],
    "pageTitle": "Flume 1.6.0 User Guide \u2014 Apache Flume",
    "title": "",
    "url": "https://flume.apache.org/FlumeUserGuide.html",
    "hasHeader": true,
    "headerPosition": "FIRST_ROW",
    "tableType": "RELATION",
    "tableNum": 25,
    "s3Link": "common-crawl/crawl-data/CC-MAIN-2015-32/segments/1438042987171.38/warc/CC-MAIN-20150728002307-00271-ip-10-236-191-2.ec2.internal.warc.gz",
    "recordEndOffset": 887563215,
    "recordOffset": 887498929,
    "tableOrientation": "HORIZONTAL",
    "TableContextTimeStampAfterTable": "{306517=If the Flume event body contained 2012-10-18 18:47:57,614 some log line and the following configuration was used, 141049=The above configuration will round down the timestamp to the last 10th minute. For example, an event with timestamp 11:54:34 AM, June 12, 2012 will cause the hdfs path to become /flume/events/2012-06-12/1150/00., 155894=The above configuration will round down the timestamp to the last 10th minute. For example, an event with timestamp header set to 11:54:34 AM, June 12, 2012 and \u2018country\u2019 header set to \u2018india\u2019 will evaluate to the partition (continent=\u2019asia\u2019,country=\u2019india\u2019,time=\u20182012-06-12-11-50\u2019. The serializer is configured to accept tab separated input containing three fields and to skip the second field., 305155=If the Flume event body contained 1:2:3.4foobar5 and the following configuration was used}",
    "lastModified": "Wed, 29 Jul 2015 21:21:07 GMT",
    "textBeforeTable": "$ bin/flume-ng agent \u2013conf conf -z zkhost:2181,zkhost1:2181 -p /flume \u2013name a1 -Dflume.root.logger=INFO,console Once the configuration file is uploaded, start the agent with following options |- /a2 [Agent config file] |- /a1 [Agent config file] - /flume  Flume supports Agent configurations via Zookeeper. This is an experimental feature. The configuration file needs to be uploaded in the Zookeeper, under a configurable prefix. The configuration file is stored in Zookeeper Node data. Following is how the Zookeeper Node tree would look like for agents a1 and a2 Zookeeper based Configuration\u00b6  Congratulations - you\u2019ve successfully configured and deployed a Flume agent! Subsequent sections cover agent configuration in much more detail.   12/06/19 15:32:34 INFO sink.LoggerSink: Event: { headers:{} body: 48 65 6C 6C 6F 20 77 6F 72 6C 64 21 0D Hello world!. } 12/06/19 15:32:19 INFO source.NetcatSource: Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:44444] 12/06/19 15:32:19 INFO source.NetcatSource: Source starting  The original Flume terminal will output the event in a log message.  OK Hello world! <ENTER> Escape character is '^]'. Connected to localhost.localdomain (127.0.0.1). Trying 127.0.0.1... $ telnet localhost 44444  From a separate terminal, we can then telnet port 44444 and send Flume an event: Note that in a full deployment we would typically include one more option: --conf=<conf-dir>. The <conf-dir> directory would include a",
    "textAfterTable": "agent_foo.sinks = hdfs-Cluster1-sink agent_foo.channels = mem-channel-1 # set channel for sources, sinks # properties of avro-AppSrv-source agent_foo.sources.avro-AppSrv-source.type = avro agent_foo.sources.avro-AppSrv-source.bind = localhost agent_foo.sources.avro-AppSrv-source.port = 10000 # properties of mem-channel-1 agent_foo.channels.mem-channel-1.type = memory agent_foo.channels.mem-channel-1.capacity = 1000 agent_foo.channels.mem-channel-1.transactionCapacity = 100 # properties of hdfs-Cluster1-sink agent_foo.sinks.hdfs-Cluster1-sink.type = hdfs agent_foo.sinks.hdfs-Cluster1-sink.hdfs.path = hdfs://namenode/flume/webdata #... Adding multiple flows in an agent\u00b6 A single Flume agent can contain several independent flows. You can list multiple sources, sinks and channels in a config. These components can be linked to form multiple flows: # list the sources, sinks and channels for the agent <Agent>.sources = <Source1> <Source2> <Agent>.sinks = <Sink1> <Sink2> <Agent>.channels = <Channel1> <Channel2> Then you can link the sources and sinks to their corresponding channels (for sources) of channel (for sinks) to setup two different flows. For example, if you need to setup two flows in an agent, one going from an external avro client to external HDFS and another from output of a tail to avro sink, then here\u2019s a config to do that: # list the sources, sinks and channels in the agent agent_foo.sources",
    "hasKeyColumn": true,
    "keyColumnIndex": 0,
    "headerRowIndex": 0
}