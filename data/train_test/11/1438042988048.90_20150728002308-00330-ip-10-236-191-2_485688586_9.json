{
    "relation": [
        [
            "Date",
            "Oct 23, 1987",
            "Apr 16, 1993",
            "Jan 7, 1997",
            "Sep 30, 1997",
            "Sep 27, 2001"
        ],
        [
            "Code",
            "AS",
            "FPAY",
            "CC",
            "FPAY",
            "FPAY"
        ],
        [
            "Event",
            "Assignment",
            "Fee payment",
            "Certificate of correction",
            "Fee payment",
            "Fee payment"
        ],
        [
            "Description",
            "Owner name: BOSTON UNIVERSITY, 881 COMMONWEALTH AVENUE, BOSTON Free format text: ASSIGNMENT OF ASSIGNORS INTEREST.;ASSIGNORS:CARPENTER, GAIL A.;GROSSBERG, STEPHEN;REEL/FRAME:004781/0820 Effective date: 19871001 Owner name: BOSTON UNIVERSITY, A CORP. OF MASSACHUSETTS,MASSAC Free format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:CARPENTER, GAIL A.;GROSSBERG, STEPHEN;REEL/FRAME:004781/0820 Effective date: 19871001",
            "Year of fee payment: 4",
            "",
            "Year of fee payment: 8",
            "Year of fee payment: 12"
        ]
    ],
    "pageTitle": "Patent US4914708 - System for self-organization of stable category recognition codes for analog ... - Google Patents",
    "title": "",
    "url": "http://www.google.com/patents/US4914708?dq=7260910",
    "hasHeader": true,
    "headerPosition": "FIRST_ROW",
    "tableType": "RELATION",
    "tableNum": 9,
    "s3Link": "common-crawl/crawl-data/CC-MAIN-2015-32/segments/1438042988048.90/warc/CC-MAIN-20150728002308-00330-ip-10-236-191-2.ec2.internal.warc.gz",
    "recordEndOffset": 485721219,
    "recordOffset": 485688586,
    "tableOrientation": "HORIZONTAL",
    "TableContextTimeStampAfterTable": "{89151=The output pathways from level F2 of ART to the postprocessor (FIG. 8) can learn to read out any spatial pattern or spatiotemporal pattern of outputs by applying theorems about associative spatial pattern learning in avalanche-type circuits (Grossberg, 1969, 1970, 1982). Thus the architecture as a whole can stably self-organize an invariant recognition code and an associative map to an arbitrary format of output patterns., 84499=In the present application, the preprocessor has been designed by using available techniques to present ART 2 with spectra that are invariant under 2-D spatial translation, dilation, and rotation (Casaent and Psaltis, 1976; Cavanagh, 1978, 1984; Szu, 1986). Using an invariant input filter such as a Fourier-Mellon filter is a familiar and direct approach to achieving 2-D spatial invariance in pattern recognition. Alternative approaches use several hierarchically organized processing stages to gradually free image processing from its spatial coordinates (Fukushima, 1980; Fukushima and Miyake, 1984; Grossberg, 1978, Section 19; reprinted in Grossberg, 1982); higher-order threshold logic units (Maxwell, Giles, and Chen, 1986); or match-gated associative mechanisms (Grossberg and Kuperstein, 1986, Chapter 6)., 46238=Adaptive resonance architectures are neural networks that self-organize stable recognition categories in real time in response to arbitrary sequences of input patterns. The basic principles of adaptive resonance theory (ART) were introduced in Grossberg, \"Adaptive pattern classification and universal recoding, II: Feedback, expectation, olfaction, and illusions.\" Biological Cybernetics, 23 (1976) 187-202. A class of adaptive resonance architectures has since been characterized as a system of ordinary differential equations by Carpenter and Grossberg, \"Category learning and adaptive pattern recognition: A neural network model,\" Proceedings of the Third Army Conference on Applied Mathematics and Computing, ARO Report 86-1 (1985) 37-56, and \"A massively parallel architecture for a self-organizing neural pattern recognition machine.\" Computer Vision, Graphics, and Image Processing, 37 (1987) 54-115. One implementation of an ART system is presented in U.S. application Ser. No. PCT/US86/02553, filed Nov. 26, 1986 by Carpenter and Grossberg for \"Pattern Recognition System\"., 45865=This research was supported in part by the Air Force Office of Scientific Research (AFOSR F49620-86-C-0037 and AFOSR 85-1049), the Army Research Office (ARO DAAG-29-85-K-0095), and the National Science Foundation (NSF DMS-86-11959 (G.A.C.) and NSF IRI-84-17756 (S.G.))., 86103=If the figure derived in this way is noisy and irregular, the vigilance parameter may be set at a value that adjusts for the expected level of noise fluctuations generated by the imaging devices. In addition, as in FIG. 9, the figure boundary may be extracted, completed, and regularized using the emergent boundary segmentation process of a Boundary Contour System (Grossberg and Mingolla, 1985, Grossberg, 1987). Or some portion of the Boundary Contour System can be used, such as its second competitive stage, which can choose the maximally activated oriented filter at each spatial location and inhibit responses from statistically unoriented regions. This completed boundary can then serve as the input pattern to ART 2. The figure interior within the emergent segmentation may also be smoothly completed using the filling-in process of a Feature Contour System (Cohen and Grossberg, 1984) and this filled-in representation used as an input source to ART 2. Combinations of the completed boundary and filled-in figural representations may also be chosen as the inputs to ART 2, thereby fully exploiting the architecture's self-scaling properties., 87274=As ART 2 self-organizes recognition categories in response to these preprocessed inputs, its categorical choices at the F2 classifying level self-stabilize through time. In examples wherein F2 makes a choice, it can be used as the first level of an ART 1 architecture, or yet another ART architecture, should one prefer. Let us call the classifying level of this latter architecture F3. Level F3 can be used as a source of pre-wired priming inputs to F2. Alternatively, self-stabilizing choices by F3 can quickly be learned in response to the choices made at F2. Then F3 can be used as a source of self-organized priming inputs to F2, and a source of priming patterns can be associated with each of the F3 choices via mechanisms of associative pattern learning (Grossberg, 1969, 1982). For this to work well, the normalizing property of F3 is important. After learning of these primes takes place, turning on a particular prime can activate a learned F3 to F2 top-down expectation. Then F2 can be supraliminally activated only by an input exemplar which is a member of the recognition category of the primed F2 node. The architecture ignores all but the primed set of input patterns. In other words, the prime causes the architecture to pay attention only to expected sources of input information. Due to the spatial invariance properties of the preprocessor, the expected input patterns can be translated, dilated, or rotated in 2-D without damaging recognition. Due to the similarity grouping properties of ART 2 at a fixed level of vigilance, suitable deformations of these input patterns, including deformations due to no more than the anticipated levels of noise, can also be recognized., 85334=Before the input pattern is processed by the invariant filter, the image figure to be recognized must be detached from the image background. This can be accomplished, for example, by using a range detector focussed at the distance of the figure, and detaching the figure from contiguous ground by spatially intersecting the range pattern with a pattern from another detector that is capable of differentiating figure from ground. A doppler image can be intersected when the figure is moving. The intensity of laser return can be intersected when the figure is stationary (Gschwendtner, Harney, and Hull, 1983; Harney, 1980, 1981; Harney and Hull, 1980; Hull and Marcus, 1980; Sullivan, 1980, Sullivan, 1980, 1981; Sullivan, Harney and Martin, 1979)., 81098=This architecture exploits properties of the ART1 adaptive resonance theory architectures which have been developed in Carpenter and Grossberg, \"Category learning and adaptive pattern recognition: A neural network model,\" Proceedings of the Third Army Conference on Applied Mathematics and Computing, 1985, ARO Report 86-1, 37-56, and \"A massively parallel architecture for a self-organizing neural pattern recognition machine,\" Computer Vision, Graphics, and Image Processing, 1987, 37, 54-115; the ART2 architecture described here; the Boundary Contour System for boundary segmentation and the Feature Contour System for figural filling-in which have been developed in Cohen and Grossberg, \"Neural dynamics of brightness perception: Features, boundaries, diffusion, and resonance,\" Perception and Psychophysics, 1984, 36, 428-456, Grossberg, 1987, and Grossberg and Mingolla, \"Neural dynamics of form perception: Boundary completion, illusory figures, and neon color spreading,\" Psychological Review, 1985, 92, 173-211, and \"Neural dynamics of perceptual groupings: Textures, boundaries, and emergent segmentations,\" Perception and Psychophysics, 1985, 38, 141-171: Theorems on associative pattern learning and associative map learning in Grossberg, \"On learning and energy-entropy dependent in recurrent and nonrecurrent signed networks,\" Journal of Statistical Physics, 1969, 1, 319-350, \"Some networks that can learn, remember, and reproduce any number of complicated space-time patterns II,\" Studies in Applied Mathematics, 1970, 49, 135-166, and \"Studies of mind and brain: neural principles of learning, preception, development, cognition, and motor control,\" Boston: Reidel Press 1982; and circuit designs to focus attention on desired goal objects by using learned feedback interactions between external sensory events and internal homeostatic events in Grossberg, \"A neural theory of punishment and avoidance, II: Quantitative theory,\" Mathematical Biosciences, 1972, 15, 253-285, \"Studies of mind and brain: Neural principles of learning, perceptionn, development, cognition, and motor control,\" Boston: Reidel Press, 1982, and The adaptive brain, I: Cognition, learning, reinforcement, and rhythm, Amsterdam: Elsevier/North-Holland, 1987. The overall circuit design embodies an intentional learning machine in which distinct cognitive, homeostatic, and motor representations are self-organized in a coordinated fashion., 89607=The model of priming patterns can be both modified and refined. The interactions (priming to ART) and (ART to postprocessor) can be modified so that output patterns are read-out only if the input patterns have yielded rewards in the past and if the machine's internal needs for these rewards have not yet been satisfied (Grossberg, 1972, 1982, 1987; Grossberg and Levine, 1987; Grossberg and Schmajuk, 1987). In this variation of the architecture, the priming patterns supply motivational signals for releasing outputs only if an input exemplar from a desired recognition category is detected.}",
    "textBeforeTable": "Patent Citations While this invention has been particularly shown and described with references to preferred embodiments thereof, it will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the spirit and scope of the invention as defined by the appended claims. For example, although the orienting subsystem may be removed, more trials may be required for the learning sequences to self-stabilize, and some erroneous groupings may be made initially. Further, one would lose the flexibility of controlling vigilance. Also, although for some systems, the F1 internal feedback may be removed, certain inputs might never stabilize to a specific category. Also, the functions included in the feedback loops can be modified to provide for any desired signal enhancement and repression. The model of priming patterns can be both modified and refined. The interactions (priming to ART) and (ART to postprocessor) can be modified so that output patterns are read-out only if the input patterns have yielded rewards in the past and if the machine's internal needs for these rewards have not yet been satisfied (Grossberg, 1972, 1982, 1987; Grossberg and Levine, 1987; Grossberg and Schmajuk, 1987). In this variation of the architecture, the priming patterns supply motivational signals for releasing outputs only if an input exemplar from a desired recognition category is detected. The output pathways from level F2 of ART to the postprocessor (FIG. 8)",
    "textAfterTable": "2 Carpenter, G. A. and Grossberg, S., A massively parallel architecture for a self-organizing neural pattern recognition machine, Computer Vision, Graphics, and Image Processing, Feb., 1986. 3 * Carpenter, G. A. and Grossberg, S., Absolutely stable learning of recognition codes by a self organizing neural network, AIP, Snowbird meeting, May 1986. 4 Carpenter, G. A. and Grossberg, S., Absolutely stable learning of recognition codes by a self-organizing neural network, AIP, Snowbird meeting, May 1986. 5 * Carpenter, G. A. and Grossberg, S., Category learning and adaptive pattern recognition: A neural network model, Proceedings of the Third Army Conference on Applied Mathematics and Computing, 1985. 6 * Carpenter, G. A. and Grossberg, S., Neural dynamics of category learning and recognition: structural invariants, reinforcement, and evoked potentials, Pattern Recognition and Concepts in Animals, People, and Machines, Aug. 1985. 7 * EP, A, 0245508 (Nippon Hoso Kyokai) 19 Nov. 1987. 8 G. A. Carpenter et al.: \"Art 2: self-organization of stable category, IEEE First International Conference on Neural Networks\", Sheraton Harbor Island East San Diego,",
    "hasKeyColumn": true,
    "keyColumnIndex": 2,
    "headerRowIndex": 0
}