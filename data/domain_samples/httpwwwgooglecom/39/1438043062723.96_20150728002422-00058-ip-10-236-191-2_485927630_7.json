{
    "relation": [
        [
            "Citing Patent",
            "US7984022 *",
            "US8135905 *",
            "US8140821 *",
            "US8156306 *",
            "US8166314",
            "US8176104",
            "US8200630 *",
            "US8200923 *",
            "US8200926 *",
            "US8209506",
            "US8261068",
            "US8280858 *",
            "US8285690 *",
            "US8315985 *",
            "US8332612",
            "US8364641",
            "US8392437 *",
            "US8392791 *",
            "US8396832",
            "US8416954",
            "US8438139 *",
            "US8443159 *",
            "US8447740 *",
            "US8452731 *",
            "US8452732",
            "US8458132",
            "US8458144 *",
            "US8458181",
            "US8478951",
            "US8489548 *",
            "US8499131 *",
            "US8549245",
            "US8607017 *",
            "US8612392",
            "US8620886 *",
            "US8626713",
            "US8639658",
            "US8660997 *",
            "US8671082 *",
            "US8675296 *",
            "US8712974 *",
            "US8751462",
            "US8793227",
            "US8799467",
            "US8799595 *",
            "US8849772",
            "US8892527",
            "US8898119",
            "US8904006",
            "US8904120 *",
            "US8904128",
            "US8935497",
            "US8959062 *",
            "US8959227",
            "US9047304",
            "US9063854 *",
            "US9094195 *",
            "US20100106691 *",
            "US20100161554 *",
            "US20100198783 *",
            "US20100332446 *",
            "US20110083019 *",
            "US20110099154 *",
            "US20110252217 *",
            "US20110307457 *",
            "US20120143832 *",
            "US20120150827 *",
            "US20120254174 *",
            "US20130018854 *",
            "US20130246366 *",
            "US20140201384 *",
            "US20140244691 *",
            "US20150046502 *",
            "US20150058583 *"
        ],
        [
            "Filing date",
            "Apr 18, 2008",
            "Jan 10, 2008",
            "Dec 18, 2009",
            "Dec 18, 2009",
            "Dec 30, 2008",
            "May 26, 2011",
            "Aug 18, 2009",
            "Dec 31, 2008",
            "May 28, 2009",
            "Sep 28, 2007",
            "Sep 30, 2008",
            "Jun 29, 2009",
            "Sep 18, 2009",
            "Dec 18, 2008",
            "Mar 5, 2012",
            "Dec 15, 2010",
            "Mar 15, 2010",
            "Aug 8, 2008",
            "Dec 8, 2010",
            "Sep 30, 2008",
            "Dec 1, 2010",
            "May 2, 2012",
            "Nov 14, 2008",
            "Sep 25, 2009",
            "May 17, 2012",
            "Jul 3, 2012",
            "Oct 22, 2009",
            "Dec 8, 2010",
            "Apr 13, 2012",
            "Apr 12, 2010",
            "Apr 13, 2010",
            "May 21, 2012",
            "Sep 14, 2012",
            "May 9, 2011",
            "Sep 20, 2011",
            "Dec 8, 2010",
            "Apr 21, 2010",
            "Aug 24, 2011",
            "Feb 26, 2009",
            "Apr 16, 2012",
            "Dec 22, 2009",
            "Nov 14, 2008",
            "Sep 7, 2012",
            "Sep 9, 2011",
            "Aug 28, 2008",
            "Nov 14, 2008",
            "Sep 14, 2012",
            "Dec 15, 2010",
            "Dec 8, 2010",
            "Dec 15, 2010",
            "Jun 8, 2011",
            "Aug 16, 2013",
            "Aug 13, 2009",
            "Apr 27, 2012",
            "Nov 28, 2011",
            "Apr 18, 2012",
            "Oct 2, 2009",
            "Sep 25, 2009",
            "Dec 22, 2009",
            "Apr 12, 2010",
            "Jun 29, 2009",
            "Oct 2, 2009",
            "Oct 22, 2009",
            "Apr 13, 2010",
            "Mar 5, 2009",
            "",
            "Aug 13, 2009",
            "Mar 31, 2011",
            "",
            "Apr 24, 2013",
            "Jan 16, 2013",
            "Feb 25, 2014",
            "Oct 24, 2014",
            "Aug 23, 2013"
        ],
        [
            "Publication date",
            "Jul 19, 2011",
            "Mar 13, 2012",
            "Mar 20, 2012",
            "Apr 10, 2012",
            "Apr 24, 2012",
            "May 8, 2012",
            "Jun 12, 2012",
            "Jun 12, 2012",
            "Jun 12, 2012",
            "Jun 26, 2012",
            "Sep 4, 2012",
            "Oct 2, 2012",
            "Oct 9, 2012",
            "Nov 20, 2012",
            "Dec 11, 2012",
            "Jan 29, 2013",
            "Mar 5, 2013",
            "Mar 5, 2013",
            "Mar 12, 2013",
            "Apr 9, 2013",
            "May 7, 2013",
            "May 14, 2013",
            "May 21, 2013",
            "May 28, 2013",
            "May 28, 2013",
            "Jun 4, 2013",
            "Jun 4, 2013",
            "Jun 4, 2013",
            "Jul 2, 2013",
            "Jul 16, 2013",
            "Jul 30, 2013",
            "Oct 1, 2013",
            "Dec 10, 2013",
            "Dec 17, 2013",
            "Dec 31, 2013",
            "Jan 7, 2014",
            "Jan 28, 2014",
            "Feb 25, 2014",
            "Mar 11, 2014",
            "Mar 18, 2014",
            "Apr 29, 2014",
            "Jun 10, 2014",
            "Jul 29, 2014",
            "Aug 5, 2014",
            "Aug 5, 2014",
            "Sep 30, 2014",
            "Nov 18, 2014",
            "Nov 25, 2014",
            "Dec 2, 2014",
            "Dec 2, 2014",
            "Dec 2, 2014",
            "Jan 13, 2015",
            "Feb 17, 2015",
            "Feb 17, 2015",
            "Jun 2, 2015",
            "Jun 23, 2015",
            "Jul 28, 2015",
            "Apr 29, 2010",
            "Jun 24, 2010",
            "Aug 5, 2010",
            "Dec 30, 2010",
            "Apr 7, 2011",
            "Apr 28, 2011",
            "Oct 13, 2011",
            "Dec 15, 2011",
            "Jun 7, 2012",
            "Jun 14, 2012",
            "Oct 4, 2012",
            "Jan 17, 2013",
            "Sep 19, 2013",
            "Jul 17, 2014",
            "Aug 28, 2014",
            "Feb 12, 2015",
            "Feb 26, 2015"
        ],
        [
            "Applicant",
            "International Business Machines Corporation",
            "Hitachi, Ltd.",
            "Emc Corporation",
            "Emc Corporation",
            "Emc Corporation",
            "International Business Machines Corporation",
            "Netapp, Inc.",
            "Emc Corporation",
            "Symantec Corporation",
            "Emc Corporation",
            "Emc Corporation",
            "Oracle America, Inc.",
            "Hitachi, Ltd.",
            "Symantec Corporation",
            "Emc Corporation",
            "International Business Machines Corporation",
            "Symantec Corporation",
            "George Saliba",
            "International Business Machines Corporation",
            "Emc Corporation",
            "International Business Machines Corporation",
            "Symantec Corporation",
            "Emc Corporation",
            "Quest Software, Inc.",
            "International Business Machines Corporation",
            "International Business Machines Corporation",
            "Oracle America, Inc.",
            "International Business Machines Corporation",
            "Emc Corporation",
            "Huawei Technologies Co., Ltd.",
            "Hewlett-Packard Development Company, L.P.",
            "Emc Corporation",
            "Netapp, Inc.",
            "International Business Machines Corporation",
            "Netapp Inc.",
            "International Business Machines Corporation",
            "Symantec Corporation",
            "International Business Machines Corporation",
            "Netapp, Inc.",
            "International Business Machines Corporation",
            "Google Inc.",
            "Emc Corporation",
            "Hitachi, Ltd.",
            "Microsoft Corporation",
            "American Megatrends, Inc.",
            "Emc Corporation",
            "Netapp, Inc.",
            "Netapp, Inc.",
            "International Business Machines Corporation",
            "Netapp Inc.",
            "Hewlett-Packard Development Company, L.P.",
            "Emc Corporation",
            "Hitachi Solutions, Ltd.",
            "International Business Machines Corporation",
            "International Business Machines Corporation",
            "American Megatrends, Inc.",
            "Andrew LEPPARD",
            "Kenneth Preslan",
            "Google Inc.",
            "Huawei Technologies Co., Ltd.",
            "Sun Microsystems, Inc.",
            "Leppard Andrew",
            "Sun Microsystems, Inc.",
            "Mark David Lillibridge",
            "Hitachi Solutions, Ltd.",
            "International Business Machines Corporation",
            "Hitachi Solutions, Ltd.",
            "Emc Corporation",
            "Netapp, Inc.",
            "Quest Software, Inc.",
            "Cisco Technology, Inc.",
            "Emc Corporation",
            "Netapp Inc.",
            "International Business Machines Corporation"
        ],
        [
            "Title",
            "Space recovery with storage management coupled with a deduplicating storage system",
            "Storage system and power consumption reduction method for switching on/off the power of disk devices associated with logical units in groups configured from the logical units",
            "Efficient read/write algorithms and associated mapping for block-level data reduction processes",
            "Systems and methods for using thin provisioning to reclaim space identified by data reduction processes",
            "Selective I/O to logical unit when encrypted, but key is not available or when encryption status is unknown",
            "Space recovery with storage management coupled with a deduplicating storage system",
            "Client data retrieval in a clustered computing network",
            "Method and apparatus for block level data de-duplication",
            "Methods and systems for creating full backups",
            "De-duplication in a virtualized storage environment",
            "Systems and methods for selective encryption of operating system metadata for host-based encryption of data at rest on a logical unit",
            "Storage pool scrubbing with concurrent snapshots",
            "Storage system for eliminating duplicated data",
            "Optimizing the de-duplication rate for a backup stream",
            "Systems and methods for using thin provisioning to reclaim space identified by data reduction processes",
            "Method and system for deduplicating data",
            "Method and system for providing deduplication information to applications",
            "Unified data protection and data de-duplication in a storage system",
            "Independent fileset generations in a clustered redirect-on-write filesystem",
            "Systems and methods for accessing storage or network based replicas of encrypted volumes with no additional key management",
            "Dynamic rewrite of files within deduplication system",
            "Methods and systems for creating full backups",
            "Stream locality delta compression",
            "Remote backup and restore",
            "Identifying modified chunks in a data set for storage",
            "Method and system for deduplicating data",
            "Data deduplication method using file system constructs",
            "Distributed free block map for a clustered redirect-on-write file system",
            "Method and apparatus for block level data de-duplication",
            "Method, system, and device for data synchronization",
            "Capping a number of locations referred to by chunk references",
            "De-duplication in a virtualized storage environment",
            "Use of similarity hash to route data for improved deduplication in a storage server cluster",
            "Identifying modified chunks in a data set for storage",
            "Host side deduplication",
            "Multiple contexts in a redirect on write file system",
            "Cache management for file systems supporting shared blocks",
            "File system object-based deduplication",
            "Use of predefined block pointers to reduce duplicate storage of certain data in a storage subsystem of a storage server",
            "Creating an identical copy of a tape cartridge",
            "Asynchronous distributed de-duplication for replicated content addressable storage clusters",
            "Delta compression after identity deduplication",
            "Storage system for eliminating duplicated data",
            "Storage and communication de-duplication",
            "Eliminating duplicate data in storage systems with boot consolidation",
            "Data replication with delta compression",
            "Use of predefined block pointers to reduce duplicate storage of certain data in a storage subsystem of a storage server",
            "Fingerprints datastore and stale fingerprint removal in de-duplication environments",
            "In-flight block map for a clustered redirect-on-write filesystem",
            "Segmented fingerprint datastore and scaling a fingerprint datastore in de-duplication environments",
            "Processing a request to restore deduplicated data",
            "De-duplication in a virtualized storage environment",
            "Data storage device with duplicate elimination function and control device for creating search index for the data storage device",
            "In-flight block map for a clustered redirect-on-write filesystem",
            "Optimization of fingerprint-based deduplication",
            "Systems and methods for cluster raid data consistency",
            "Protecting de-duplication repositories against a malicious attack",
            "Remote backup and restore",
            "Asynchronous distributed de-duplication for replicated content addressable storage clusters",
            "Method, system, and device for data synchronization",
            "Storage pool scrubbing with concurrent snapshots",
            "Protecting de-duplication repositories against a malicious attack",
            "Data Deduplication Method Using File System Constructs",
            "Capping a number of locations referred to by chunk references",
            "Integrated duplicate elimination system, data storage device, and server device",
            "Dynamic rewrite of files within deduplication system",
            "Data storage device with duplicate elimination function and control device for creating search index for the data storage device",
            "Time-based data partitioning",
            "Use of similarity hash to route data for improved deduplication in a storage server cluster",
            "Remote backup and restore",
            "Method for optimizing wan traffic with efficient indexing scheme",
            "Cluster storage using subsegmenting for efficient storage",
            "Migrating data from legacy storage systems to object storage systems",
            "System and method for improved placement of blocks in a deduplication-erasure code environment"
        ]
    ],
    "pageTitle": "Patent US7747584 - System and method for enabling de-duplication in a storage system architecture - Google Patents",
    "title": "",
    "url": "http://www.google.com/patents/US7747584?dq=5311516",
    "hasHeader": true,
    "headerPosition": "FIRST_ROW",
    "tableType": "RELATION",
    "tableNum": 7,
    "s3Link": "common-crawl/crawl-data/CC-MAIN-2015-32/segments/1438043062723.96/warc/CC-MAIN-20150728002422-00058-ip-10-236-191-2.ec2.internal.warc.gz",
    "recordEndOffset": 485982543,
    "recordOffset": 485927630,
    "tableOrientation": "HORIZONTAL",
    "TableContextTimeStampBeforeTable": "{6151=The file system 360 implements a virtualization system of the storage operating system 300 through the interaction with one or more virtualization modules illustratively embodied as, e.g., a virtual disk (vdisk) module (not shown) and a SCSI target module 335. The vdisk module enables access by administrative interfaces, such as a user interface of a management framework 1110 (see FIG. 11), in response to a user (system administrator) issuing commands to the node 200. The SCSI target module 335 is generally disposed between the FC and iSCSI drivers 328, 330 and the file system 360 to provide a translation layer of the virtualization system between the block (lun) space and the file system space, where luns are represented as blocks.}",
    "TableContextTimeStampAfterTable": "{138693=After the data directed to the first stripe has been written to disk of DV 1, the de-duplication technique may be initiated to eliminate duplicate data for the affected file. As noted above, the local D-module serving DV 1 retains a copy of the data in its memory (i.e., a content identifier) so that it can perform the novel Locate by content function 385. For those blocks for which it is responsible, the local D-module executes the Locate by content function to calculate a checksum for each (modified) 4 KB block of data in order to generate a de-dup marker hash value (Step 1916). In Step 1918, the de-dup marker value determines the constituent volume of the SVS that should be selected to store the data, i.e., whether the data should be stored locally on the volume served by the local D-module. If the de-dup hash value indicates that the data should be locally stored, the local D-module performs the de-dup technique in Step 1920. If the de-dup hash value indicates that the data should be remotely stored on another constituent SVS volume, the local D-module sends the modified data to remote D-module serving that volume (Step 1922) and the procedure continues as described above., 144383=However, if the indirect block does contain a de-dup marker, the local D-module examines the marker to determine to which remote D-module the request should be forwarded for serving the request (Step 2020). As noted, the de-dup marker comprises a special value that represents the result of the Locate by content function 385. For example, the value of the de-dup marker may indicate that the data block is not stored on DV1 and serviced by the local D-module, but rather is stored on DV4 of the SVS and serviced by a remote D-module. In Step 2022, the local D-module forwards the request to the remote D-module to retrieve the data block. In Step 2024, the remote D-module forwards the requested data to the local D-module which, in Step 2026 returns the requested data to the N-module. The procedure then ends at Step 2028., 139969=In Step 1934, the local D-module forwards the remaining data to another remote D-module serving a second constituent SVS volume (e.g., DV2) which, in Step 1936, performs the cross-stripe operation for a second stripe affected by the operation, i.e., the next data volume storing the next stripe in accordance with the file striping algorithm associated with the SVS. Once the operation of Step 1936 (as well as Steps 1920 and 1926) completes, the local D-module removes the range lock in Step 1938 and the procedure continues to Step 1940 where the local D-module returns success to the N-module which, in turn, returns an indication of success back to its caller (client). The procedure then ends at Step 1942., 46724=An example of a technique for eliminating duplicate data is described in U.S. patent application Ser. No. 11/105,895, filed on Apr. 13, 2005, entitled METHOD AND APPARATUS FOR IDENTIFYING AND ELIMINATING DUPLICATE DATA BLOCKS AND SHARING DATA BLOCKS IN A STORAGE SYSTEM, by Ling Zheng, et al, the contents of which are hereby incorporated by reference. Here, data de-duplication operations are performed on fixed size blocks. When a new block is to be stored, a hash value is computed as a fingerprint of the block. The fingerprint is then compared with a hash table containing fingerprints of previously stored blocks. If the new block's fingerprint is identical to that of a previously stored block, there is a high degree of probability that the new block is identical to the previously stored block. In such a case, the two blocks are compared to test whether they are indeed identical. If so, the new block is replaced with a pointer to the previously stored block, thereby reducing storage resource consumption., 90141=When an on-disk inode (or block) is loaded from disk 130 into memory 224, its corresponding in-core structure embeds the on-disk structure. For example, the dotted line surrounding the inode 600 indicates the in-core representation of the on-disk inode structure. The in-core structure is a block of memory that stores the on-disk structure plus additional information needed to manage data in the memory (but not on disk). The additional information may include, e.g., a \u201cdirty\u201d bit 670. After data in the inode (or block) is updated/modified as instructed by, e.g., a write operation, the modified data is marked \u201cdirty\u201d using the dirty bit 670 so that the inode (block) can be subsequently \u201cflushed\u201d (stored) to disk. The in-core and on-disk format structures of the WAFL file system, including the inodes and inode file, are disclosed and described in U.S. Pat. No. 5,819,292 titled METHOD FOR MAINTAINING CONSISTENT STATES OF A FILE SYSTEM AND FOR CREATING USER-ACCESSIBLE READ-ONLY COPIES OF A FILE SYSTEM by David Hitz et al., issued on Oct. 6, 1998., 104892=The VLDB 1130 is a database process that tracks the locations of various storage components (e.g., SVSs, flexible volumes, aggregates, etc.) within the cluster 100 to thereby facilitate routing of requests throughout the cluster. In the illustrative embodiment, the N-module 310 of each node accesses a configuration table 235 that maps the SVS ID 502 of a data container handle 500 to a D-module 350 that \u201cowns\u201d (services) the data container within the cluster. The VLDB includes a plurality of entries which, in turn, provide the contents of entries in the configuration table 235; among other things, these VLDB entries keep track of the locations of the flexible volumes (hereinafter generally \u201cvolumes 910\u201d) and aggregates 900 within the cluster. Examples of such VLDB entries include a VLDB volume entry 1200 and a VLDB aggregate entry 1300., 89257=Moreover, if the size of the data is greater than 64 KB but less than or equal to 64 megabytes (MB), then each pointer in the data section 660 of the inode (e.g., a second level inode) references an indirect block (e.g., a first level L1 block) that contains 1024 pointers, each of which references a 4 KB data block on disk. For file system data having a size greater than 64 MB, each pointer in the data section 660 of the inode (e.g., a third level L3 inode) references a double-indirect block (e.g., a second level L2 block) that contains 1024 pointers, each referencing an indirect (e.g., a first level L1) block. The indirect block, in turn, that contains 1024 pointers, each of which references a 4 KB data block on disk. When accessing a file, each block of the file may be loaded from disk 130 into the memory 224., 107058=The VLDB illustratively implements a RPC interface, e.g., a Sun RPC interface, which allows the N-module 310 to query the VLDB 1130. When encountering contents of a data container handle 500 that are not stored in its configuration table, the N-module sends an RPC to the VLDB process. In response, the VLDB 1130 returns to the N-module the appropriate mapping information, including an ID of the D-module that owns the data container. The N-module caches the information in its configuration table 235 and uses the D-module ID to forward the incoming request to the appropriate data container. All functions and interactions between the N-module 310 and D-module 350 are coordinated on a cluster-wide basis through the collection of management processes and the RDB library user mode applications 1100., 137825=In Step 1930, the local D-module then stores (writes) the entire data associated with the cross-stripe write operation on data volume DV1. In Step 1932, a determination is made as to which data is directed to the first stripe. Specifically, the local D-module modifies and writes that portion of the data directed to the first stripe of the file to its proper stripe location on DV 1 in accordance with a write operation (Step 1914). The remaining portion of the data is stored in an area of DV1 that is otherwise sparse in accordance with the striping algorithm employed by the SVS. By storing the remaining portion of the data, i.e., the excess data, on the sparse area of DV1, the cross-stripe write operation is atomically performed and considered \u201ccommitted\u201d to persistent storage., 102076=Specifically, each flexible volume 1050 has the same inode file structure/content as the aggregate, with the exception that there is no owner map and no WAFL/fsid/filesystem file, storage label file directory structure in a hidden meta-data root directory 1080. To that end, each flexible volume 1050 has a volinfo block 1052 that points to one or more fsinfo blocks 1054, each of which may represent a snapshot, along with the active file system of the flexible volume. Each fsinfo block, in turn, points to an inode file 1060 that, as noted, has the same inode structure/content as the aggregate with the exceptions noted above. Each flexible volume 1050 has its own inode file 1060 and distinct inode space with corresponding inode numbers, as well as its own root (fsid) directory 1070 and subdirectories of files that can be exported separately from other flexible volumes., 101349=In addition to being embodied as a container file having level 1 blocks organized as a container map, the filesystem file 1040 includes block pointers that reference various file systems embodied as flexible volumes 1050. The aggregate 1000 maintains these flexible volumes 1050 at special reserved inode numbers. Each flexible volume 1050 also has special reserved inode numbers within its flexible volume space that are used for, among other things, the block allocation bitmap structures. As noted, the block allocation bitmap structures, e.g., active map 1062, summary map 1064 and space map 1066, are located in each flexible volume., 133552=In Step 1910, the VSM 370 of the local D-module obtains (acquires) a range lock for the affected region of the file, i.e., for the entire size/length of the write data associated with the operation. In Step 1912, the local D-module determines whether the write data of the operation fits onto a single stripe (the first stripe) by examining the length of the data, the stripe width and the location within the stripe at which the write operation begins. If the write data fits within the stripe, the procedure branches to Step 1914 where a write operation is performed. Such a write operation is described in U.S. patent application Ser. No. 11/119,278 entitled STORAGE SYSTEM ARCHITECTURE FOR STRIPING DATA CONTAINER CONTENT ACROSS VOLUMES OF A CLUSTER., 114024=Each set of striping rules 1530 illustratively includes a stripe width field 1510, a stripe algorithm ID field 1515, an ordered list of volumes field 1520 and, in alternate embodiments, additional fields 1525. The striping rules 1530 contain information for identifying the organization of a SVS. For example, the stripe algorithm ID field 1515 identifies a striping algorithm used with the SVS. In the illustrative embodiment, multiple striping algorithms could be used with a SVS; accordingly, stripe algorithm ID is needed to identify which particular algorithm is utilized. Each striping algorithm, in turn, specifies the manner in which file content is apportioned as stripes across the plurality of volumes of the SVS. The stripe width field 1510 specifies the size/width of each stripe. The ordered list of volumes field 1520 contains the IDs of the volumes comprising the SVS. In an illustrative embodiment, the ordered list of volumes includes a plurality of tuples comprising a flexible volume ID and the aggregate ID storing the flexible volume. Moreover, the ordered list of volumes may specify the function and implementation of the various volumes and striping rules of the SVS. For example, the first volume in the ordered list may denote the MDV of the SVS, whereas the ordering of volumes in the list may denote the manner of implementing a particular striping algorithm, e.g., round-robin., 103050=The storage label file 1090 contained within the hidden meta-data root directory 1030 of the aggregate is a small file that functions as an analog to a conventional raid label. A raid label includes physical information about the storage system, such as the volume name; that information is loaded into the storage label file 1090. Illustratively, the storage label file 1090 includes the name 1092 of the associated flexible volume 1050, is the online/offline status 1094 of the flexible volume, and other identity and state information 1096 of the associated flexible volume (whether it is in the process of being created or destroyed)., 126806=The present invention augments this primary hashing system to ensure that identical blocks of data are stored on the storage served by the same D-module. Once identical blocks of data are written to the same aggregate, a de-duplication technique may be invoked to eliminate duplicate data and ensure that only one copy of that data is actually written to disk. An example of a technique for eliminating duplicate data that may be advantageously used with the present invention is described in the above incorporated U.S. patent application Ser. No. 11/105,895, filed on Apr. 13, 2005, entitled METHOD AND APPARATUS FOR IDENTIFYING AND ELIMINATING DUPLICATE DATA BLOCKS AND SHARING DATA BLOCKS IN A STORAGE SYSTEM, by Ling Zheng, et al., 81514=Further to the illustrative embodiment, the N-module 310 and D-module 350 are implemented as separately-scheduled processes of storage operating system 300; however, in an alternate embodiment, the modules may be implemented as pieces of code within a single operating system process. Communication between an N-module and D-module is thus illustratively effected through the use of message passing between the modules although, in the case of remote communication between an N-module and D-module of different nodes, such message passing occurs over the cluster switching fabric 150. A known message-passing mechanism provided by the storage operating system to transfer information between modules (processes) is the Inter Process Communication (IPC) mechanism. The protocol used with the IPC mechanism is illustratively a generic file and/or block-based \u201cagnostic\u201d CF protocol that comprises a collection of methods/functions constituting a CF application programming interface (API). Examples of such an agnostic protocol are the SpinFS and SpinNP protocols available from Network Appliance, Inc. The SpinFS protocol is described in the above-referenced U.S. Patent Application Publication No. US 2002/0116593., 107950=To that end, the management processes have interfaces to (are closely coupled to) RDB 1150. The RDB comprises a library that provides a persistent object store (storing of objects) for the management data processed by the management processes. Notably, the RDB 1150 replicates and synchronizes the management data object store access across all nodes 200 of the cluster 100 to thereby ensure that the RDB database image is identical on all of the nodes 200. At system startup, each node 200 records the status/state of its interfaces and IP addresses (those IP addresses it \u201cowns\u201d) into the RDB database., 115527=As noted, a Locate by offset function 375 is provided that enables the VSM 370 and other modules (such as those of N-module 310) to locate a D-module 350 and its associated volume of a SVS 1400 in order to service an access request to a file. The Locate by offset function takes as arguments, at least (i) a SVS ID 1505, (ii) an offset within the file, (iii) the inode number for the file and (iv) a set of striping rules 1530, and returns a file index that specifies the volume 910 on which that offset begins within the SVS 1400. For example, assume a data access request directed to a file is issued by a client 180 and received at the N-module 310 of a node 200, where it is parsed through the multi-protocol engine 325 to the appropriate protocol server of N-module 310., 134367=After the data has been modified and written to disk at DV 1, the de-duplication technique may be initiated to eliminate duplicate data for the affected file. Note that the local D-module serving DV1 retains a copy of the data in its memory so that it can perform the novel Locate by content function 385. In Step 1916, the local D-module exec cutes the Locate by content function to calculate a checksum for each (modified) 4 KB block of data in order to generate a de-dup marker hash value. As noted, the de-dup marker comprises a special hash value that determines, as a result of the Locate by content function, the constituent volume of the SVS that should be selected to store the data, i.e., whether the data should be stored locally on the volume served by the local D-module (Step 1918). If the de-dup hash value indicates that the data should be locally stored, the local D-module performs the de-dup technique in Step 1920, i.e., by freeing the duplicate data block., 136904=However, if the write data does not fit within the stripe (Step 1912), then in Step 1928, the VSM 370 of the local D-module records a persistent reminder, e.g., a persistent marker, denoting that a cross stripe write operation is in progress. Such a cross stripe write operation is described in U.S. patent application Ser. No. 11/119,279 entitled SYSTEM AND METHOD FOR IMPLEMENTING ATOMIC CROSS-STRIPE WRITE OPERATIONS IN A STRIPED VOLUME SET, which application is hereby incorporated by reference. In the illustrative embodiment, the persistent marker may be stored on local storage 230 of the local D-module. In the event of a crash or other failure condition to the node and/or cluster, the persistent marker \u201creminds\u201d the VSM that a cross-stripe write operation is in progress and, thus, enables the VSM to subsequently recover and complete the operation., 116433=To determine the location of a D-module 350 to which to transmit a CF message 400, the N-module 310 may first retrieve a SVS entry 1500 to acquire the striping rules 1530 (and list of volumes 1520) associated with the SVS. The N-module 310 then executes the Locate by offset function 375 to identify the appropriate volume to which to direct an operation. Thereafter, the N-module may retrieve the appropriate VLDB volume entry 1200 to identify the aggregate containing the volume and the appropriate VLDB aggregate entry 1300 to ultimately identify the appropriate D-module 350. The protocol server of N-module 310 then transmits the CF message 400 to the D-module 350., 111888=In contrast, each DV 1410, 1415 stores only file (F) inodes 1425, 1435, 1445 and ACL inode 1440. According to the inventive architecture, a DV does not store directories or other device inodes/constructs, such as symbolic links; however, each DV does store F inodes, and may store cached copies of ACL inodes, that are arranged in the same locations as their respective inodes in the MDV 1405. A particular DV may not store a copy of an inode until an I/O request for the data container associated with the inode is received by the D-module serving a particular DV. Moreover, the contents of the files denoted by these F inodes are periodically sparse according to SVS striping rules, as described further herein. In addition, since one volume is designated the CAV for each file stored on the SVS 1400, DV 1415 is designated the CAV for the file represented by inode 1425 and DV 1410 is the CAV for the files identified by inodes 1435, 1445. Accordingly, these CAVs cache certain, rapidly-changing attribute meta-data (M) associated with those files such as, e.g., file size 615, as well as access and/or modification time stamps 620., 121286=In accordance with an illustrative round robin striping algorithm, volume A 1605 contains a stripe of file content or data (D1) 1620 followed, in sequence, by two stripes of sparseness (S) 1622, 1624, another stripe of data (D4) 1626 and two stripes of sparseness (S) 1628, 1630. Volume B 1610, on the other hand, contains a stripe of sparseness (S) 1632 followed, in sequence, by a stripe of data (D2) 1634, two stripes of sparseness (S) 1636, 1638, another stripe of data (D5) 1640 and a stripe of sparseness (S) 1642. Volume C 1615 continues the round robin striping pattern and, to that end, contains two stripes of sparseness (S) 1644, 1646 followed, in sequence, by a stripe of data (D3) 1648, two stripes of sparseness (S) 1650, 1652 and another stripe of data (D6) 1654. By utilizing the sparse file implementation of the present invention, each stripe of data is located at the appropriate offset within the SVS, i.e., D1 located in the first stripe, D2 at the second, etc., 135419=If the de-dup hash value indicates that the data should be remotely stored on another constituent SVS volume, the local D-module sends the modified data to a remote D-module serving that volume (Step 1922), instructing it to store the write data at the corresponding offset within the affected file. On the local D-module, a de-dup marker is written at the corresponding lowest-level indirect block of the affected data (Step 1924) to indicate where the data can be found. For example, assume the local D-module hashes a modified block in accordance with the Locate by content function 385 and determines that the block should be stored remotely (e.g., on DV3). The local D-module thus forwards the block of data to the remote D-module serving DV3, specifying the offset at which to write the data within the file, and records a de-dup marker on its local disk. The remote D-module then performs the de-dup technique in Step 1926. Note that as part of this operation, the remote D-module could determine that, considering the content of the block, the data should be stored by yet another remote D-module on, e.g., DV4. As a result, the remote D-module sends the block to the D-module serving DV4 and records a de-dup marker indicating that it received the data and stored it on DV4 and, furthermore, that it acknowledged back to the local D-module that the write operation was successful., 143483=Upon receiving the request, the selected D-module accesses (loads) the inode corresponding to the file from the aggregate in Step 2010 and, in Step 2012, iteratively loads indirect blocks in order to find the data block that contains the requested data. In Step 2014, the D-module examines the lowest-level indirect block (or possibly the inode itself if there are no indirect blocks) to determine whether the indirect block contains a de-dup marker instead of a pointer to a block number within the aggregate (Step 2016). If the indirect block does not contain a de-dup marker, the D-module retrieves the requested data block from the aggregate in Step 2018 and returns the data in a response to the N-module in Step 2026. The N-module then returns the data to the caller (client) and the procedure ends at Step 2028.}",
    "textBeforeTable": "Patent Citations The foregoing description has been directed to particular embodiments of this invention. It will be apparent, however, that other variations and modifications may be made to the described embodiments, with the attainment of some or all of their advantages. Specifically, it should be noted that the principles of the present invention may be implemented in non-distributed file systems. Furthermore, while this description has been written in terms of N and D-modules, the teachings of the present invention are equally suitable to systems where the functionality of the N and D-modules are implemented in a single system. Alternately, the functions of the N and D-modules may be distributed among any number of separate systems, wherein each system performs one or more of the functions. Additionally, the procedures, processes and/or modules described herein may be implemented in hardware, software, embodied as a computer-readable medium having program instructions, firmware, or a combination thereof. Accordingly this description is to be taken only by way of example and not to otherwise limit the scope of the invention. Therefore, it is the object of the appended claims to cover all such variations and modifications as come within the true spirit and scope of the invention. While there has been shown and described illustrative embodiments of a system and method for enabling de-duplication in a storage system architecture comprising one or more volumes distributed across a plurality of nodes interconnected as",
    "textAfterTable": "Ousterhout, John et al., Beating the I/O Bottleneck: A Case for Log-Structured File Systems, Technical Report, Computer Science Division, Electrical Engineering and Computer Sciences, University of California at Berkeley, Oct. 30, 1988, 18 pages. 26 Patterson, D., et al., A Case for Redundant Arrays of Inexpensive Disks (RAID), SIGMOD International Conference on Management of Data, Chicago, IL, USA, Jun. 1-3, 1988, SIGMOD Record (17):3:109-16 (Sep. 1988). 27 Patterson, D., et al., A Case for Redundant Arrays of Inexpensive Disks (RAID), Technical Report, CSD-87-391, Computer Science Division, Electrical Engineering and Computer Sciences, University of California at Berkeley (1987), 26 pages. 28 Peterson, Zachary Nathaniel Joseph, Data Placement for Copy-on-Write Using Virtual Contiguity, University of CA, Santa Cruz, Master of Science in Computer Science Thesis, Sep. 2002. 29 Quinlan, Sean, A Cached WORM File System, Software-Practice and Experience, 21(12):1289-1299 (1991). 30 Rosenblum, Mendel, et al. The Design and Implementation of a Log-Structured File System Jul. 24, 1991 pp. 1-15. 31 Rosenblum, Mendel, et al., The Design and Implementation of a Log-Structured File System, in Proceedings of ACM Transactions on Computer Systems, (10)1:26-52, Feb. 1992. 32 Rosenblum, Mendel,",
    "hasKeyColumn": false,
    "keyColumnIndex": -1,
    "headerRowIndex": 0
}