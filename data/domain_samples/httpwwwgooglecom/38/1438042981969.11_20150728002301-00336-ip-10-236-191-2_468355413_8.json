{
    "relation": [
        [
            "Date",
            "Aug 22, 2000",
            "Oct 14, 2003",
            "Feb 2, 2007",
            "Jan 26, 2011",
            "Dec 9, 2014",
            "Jan 27, 2015"
        ],
        [
            "Code",
            "AS",
            "CC",
            "FPAY",
            "FPAY",
            "AS",
            "FPAY"
        ],
        [
            "Event",
            "Assignment",
            "Certificate of correction",
            "Fee payment",
            "Fee payment",
            "Assignment",
            "Fee payment"
        ],
        [
            "Description",
            "Owner name: MICROSOFT CORPORATION, WASHINGTON Free format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:SZELISKI, RICHARD S.;SALESIN, DAVID;SCHODL, ARNO;REEL/FRAME:011028/0601 Effective date: 20000816",
            "",
            "Year of fee payment: 4",
            "Year of fee payment: 8",
            "Owner name: MICROSOFT TECHNOLOGY LICENSING, LLC, WASHINGTON Free format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:MICROSOFT CORPORATION;REEL/FRAME:034541/0001 Effective date: 20141014",
            "Year of fee payment: 12"
        ]
    ],
    "pageTitle": "Patent US6611268 - System and process for generating 3D video textures using video-based ... - Google Patents",
    "title": "",
    "url": "http://www.google.com/patents/US6611268?dq=5,664,133",
    "hasHeader": true,
    "headerPosition": "FIRST_ROW",
    "tableType": "RELATION",
    "tableNum": 8,
    "s3Link": "common-crawl/crawl-data/CC-MAIN-2015-32/segments/1438042981969.11/warc/CC-MAIN-20150728002301-00336-ip-10-236-191-2.ec2.internal.warc.gz",
    "recordEndOffset": 468399667,
    "recordOffset": 468355413,
    "tableOrientation": "HORIZONTAL",
    "TableContextTimeStampAfterTable": "{158095=[2] M. Hansen, P. Anandan, K. Dana, G. van der Wal, and P. Burt. Real-time scene stabilization and mosaic construction. In Image Understanding Workshop, pages 457-465, Monterey, Calif., November 1994. Morgan Kaufmann Publishers., 157913=[1] Charles E. Jacobs, Adam Finkelstein, and David H. Salesin. Fast multiresolution image querying. Proceedings of SIGGRAPH 95, pages 277-286, August 1995., 158855=[5] R. Szeliski and H.-Y. Shum. Creating fullview panoramic image mosaics and texture-mapped models. In Computer Graphics (SIGGRAPH'97) Proceedings, pages 251-258, Los Angeles, August 1997. ACM SIGGRAPH., 159099=[6] T. Kanade, P. W. Rander, and P. J. Narayanan. Virtualized reality: constructing virtual worlds from real scenes. IEEE MultiMedia Magazine, 1(1): 34-47, January-March 1997., 154026=Generally, the generation of a 3D Video Texture involves mapping a video texture created from an input video clip onto a 3D surface extracted from at least two such images using stereo matching techniques. Referring to FIG. 17, this 3D Video Texture can be constructed by first simultaneously videotaping an object (which could be a person) from two or more different cameras positioned at different locations (process action 1700). For example, in a tested embodiment of this system, three different video cameras were placed facing a subject at about 20 degrees apart. Video from one of the cameras is used to extract, analyze and synthesize a video sprite of the object of interest using the previously described methods (process action 1702). In the tested embodiment, the video was taken from the center camera. In addition, the first frames captured by each camera (or at least two of them) are used to estimate a 3D depth map of the scene (process action 1704). This can be accomplished using conventional geometry-based modeling methods, or, as an alternative, using a conventional image-based modeling technique. The background of the scene contained within the depth map is then masked out using a background subtraction procedure and a clear shot of the scene background taken before filming of the object began, leaving just a 3D surface representation of the object (process action 1706). To generate each new frame in the 3D video animation, the extracted region making up a \u201cframe\u201d of the video sprite is mapped onto the previously generated 3D surface (process action 1708). The resulting image is rendered from a novel viewpoint (process action 1710), and then mapped into an appropriate 3D scene depiction, as indicated by process action 1712. This depiction could be a flat image of the aforementioned separately filmed background which has been warped to the correct location [7], or it could be a depiction of a new scene created to act as the background for the 3D video texture., 158350=[3] H.-Y. Shum and R. Szeliski. Construction and refinement of panoramic mosaics with global and local alignment. In Sixth International Conference on Computer Vision (ICCV'98), pages 953-958, Bombay, January 1998., 159301=[7] J. Shade, S. Gortler; L.-W. He, and R. Szeliski. Layered depth images. In Computer Graphics (SIGGRAPH'98) Proceedings, pages 231-242, Orlando, July 1998. ACM SIGGRAPH., 127166=Instead of simply jumping from one frame to another when a transition is made, the images of the sequence before and after the transition can be blended together via conventional blending methods. The second sequence is gradually blended into the first, while both sequences are running. FIG. 14 shows an example of this process, which is called crossfading.In this figure, the numbers inside the boxes represent frame numbers or combinations (blends) of frame numbers. Generally, in crossfading, frames from the sequence near the source of the transition are linearly faded out as the frames from the sequence near the destination are faded in. The fade is positioned so that it is halfway complete where the transition was scheduled. For example, referring to FIG. 14, the last three frames 1400-1402 of the video sequence prior to an unacceptable transition are respectively blended with the first three frames 1403-1405 of the video sequence after the transition. The ratio formula used dictates that last frame 1400 of the prior video sequence accounts for one-quarter of the blended frame 1406 with the third frame 1405 of the subsequent sequence accounting for three-quarters of the blended frame. The two middle frames 1401, 1404 of the sequence are blended equally to produce blended frame 1407. And finally, the third to last frame 1402 of the prior video sequence accounts for three-quarters of the blended frame 1408 with the first frame 1403 of the subsequent sequence accounting for one-quarter of the blended frame., 119225=Referring to FIG. 11, the video loop synthesis procedure begins by identifying a set of primitive loops that are to be used to construct the compound loops for the aforementioned dynamic programming table (process action 1100). This would preferably entail selecting the primitive loops remaining after the previously-described pruning procedure. In process action 1102, each identified primitive loop is placed in the appropriate cell in the table (i.e., row l, column n or (l,n)). Next, the top leftmost cell is selected (process action 1104). All loops of shorter length in that same column are identified (which in the instance of the first cell is none), and an attempt is made to combine it/them with loops from columns whose range overlaps the column being considered (process action 1106). This ensures that the created compound loops are actually playable, since the ranges of the constituent loops must overlap. The attempted combination with the lowest total cost becomes the new entry for the cell (process action 1108). This process is then repeated for each successive cell by moving through the table in a top-to-bottom, left-to-right pattern, until the last cell is reached (process actions 1110 and 1112). For example, the entry in row 5 column C is obtained by combining the entry in row 3 column C with the entry in row 2 column D, which is possible since primitive loops C and D have ranges that overlap and have lengths that sum to 5. The combination with the lowest total cost becomes the new entry., 121364=After finding the list of primitive loops in the lowest cost compound loop for a particular loop length, the primitive loops (or transitions) are scheduled in some order so that they form a valid compound loop as described above. This is preferably done in accordance with the process outlined in FIG. 12 and visualized in the example shown in FIG. 13. The process begins by scheduling any one of the primitive loops and removing it from the set of jumps to be scheduled; as outlined in process action 1200. In the example shown in FIG. 13, the chosen loop is A. Next, it is noted whether the removal of the last scheduled primitive loop breaks the remaining primitive loops into one or more sets of continuous frames, as outlined in process action 1202. In FIG. 13, the removal of A breaks the remaining loops into two continuous-range sets, namely {C,D} and {B}. The next primitive loop is then scheduled from the remaining loops that have their backwards transition after the beginning point of the last scheduled primitive, loop, but within the same covered range of frames and before any break in the continuous range of frames caused by the removal of the last scheduled primitive loop (process action 1204). In the example of FIG. 13, C is the only primitive loop that meets these criteria. The above-described primitive loop always exists, otherwise the removed loop would not have overlapped the first set and the overlapped range would not have been continuous to start with. Once scheduled, the primitive loop is eliminated from the set of loops still to be scheduled. It is next determined if the just scheduled jump is the last one within its range of covered frames, which means that it was the jump covering all its frames (process action 1206). If not, then process actions 1202 and 1204 are repeated until the last scheduled primitive loop is the last one within its range of covered frames. In the example of FIG. 13, D would be removed in the next iteration of process actions 1202 and 1204. When the last scheduled primitive loop is the last one within its range of covered frames, the process continues by determining if there are any remaining primitive loops to be scheduled (process action 1208). If so, the first occurring of the remaining sequence(s) of frames is identified (process action 1210) and the entire process (i.e., actions 1200 through 1210) is repeated until there are no more primitive loops to schedule. In the example of FIG. 13, B is the only primitive loop left to schedule. At the point where there are no more primitive loops to schedule, the procedure is complete. In the example depicted in FIG. 13, loops are scheduled in the order A-C-D-B., 158598=[4] D. Beymer. Feature correspondence by interleaving shape and texture computations. In IEEE Computer Society Conference on Computer Vision and Pattem Recognition (CVPR'96), pages 921-928, San Francisco, Calif., June 1996., 27852=This application is a continuation-in-part of a prior application entitled \u201cVideo-Based Rendering\u201d which was assigned Ser. No. 09/583,313 and filed May 30, 2000.}",
    "textBeforeTable": "Patent Citations [7] J. Shade, S. Gortler; L.-W. He, and R. Szeliski. Layered depth images. In Computer Graphics (SIGGRAPH'98) Proceedings, pages 231-242, Orlando, July 1998. ACM SIGGRAPH. [6] T. Kanade, P. W. Rander, and P. J. Narayanan. Virtualized reality: constructing virtual worlds from real scenes. IEEE MultiMedia Magazine, 1(1): 34-47, January-March 1997. [5] R. Szeliski and H.-Y. Shum. Creating fullview panoramic image mosaics and texture-mapped models. In Computer Graphics (SIGGRAPH'97) Proceedings, pages 251-258, Los Angeles, August 1997. ACM SIGGRAPH. [4] D. Beymer. Feature correspondence by interleaving shape and texture computations. In IEEE Computer Society Conference on Computer Vision and Pattem Recognition (CVPR'96), pages 921-928, San Francisco, Calif., June 1996. [3] H.-Y. Shum and R. Szeliski. Construction and refinement of panoramic mosaics with global and local alignment. In Sixth International Conference on Computer Vision (ICCV'98), pages 953-958, Bombay, January 1998. [2] M. Hansen, P. Anandan, K. Dana, G. van der Wal, and P. Burt. Real-time scene stabilization and mosaic construction. In Image Understanding Workshop, pages 457-465, Monterey, Calif., November 1994. Morgan Kaufmann Publishers. [1] Charles E. Jacobs, Adam Finkelstein, and David H. Salesin. Fast multiresolution image querying. Proceedings of SIGGRAPH 95, pages 277-286, August 1995. REFERENCES Adding sound to video textures is relatively straightforward. In essence, sound samples are associated with each frame and played back with the video frames selected to be rendered. To mask any popping effects, the same multi-way cross-fading technique described previously in",
    "textAfterTable": "US6957392 * Mar 5, 2002 Oct 18, 2005 Laszlo Systems, Inc. Interface engine providing a continuous user interface US7050655 * Aug 13, 2001 May 23, 2006 Nevengineering, Inc. Method for generating an animated three-dimensional video head US7173623 May 9, 2003 Feb 6, 2007 Microsoft Corporation System supporting animation of graphical display elements through animation object instances US7203702 * Dec 14, 2001 Apr 10, 2007 Sony France S.A. Information sequence extraction and building apparatus e.g. for producing personalised music title sequences US7257272 Apr 16, 2004 Aug 14, 2007 Microsoft Corporation Virtual image generation US7262775 Oct 24, 2003 Aug 28, 2007 Microsoft Corporation System supporting animation of graphical display elements through animation object instances US7292735 Apr 16, 2004 Nov 6, 2007 Microsoft Corporation Virtual image artifact detection US7764849 Jul 31, 2006 Jul 27, 2010 Microsoft Corporation User interface",
    "hasKeyColumn": true,
    "keyColumnIndex": 2,
    "headerRowIndex": 0
}