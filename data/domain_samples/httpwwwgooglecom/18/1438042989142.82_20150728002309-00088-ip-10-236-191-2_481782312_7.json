{
    "relation": [
        [
            "Citing Patent",
            "US6477260 *",
            "US6618155 *",
            "US6628819 *",
            "US6798897 *",
            "US6868191 *",
            "US7020347 *",
            "US7342963 *",
            "US7382374 *",
            "US7505048 *",
            "US7613321 *",
            "US8073277 *",
            "US8167430",
            "US8170283",
            "US8175333",
            "US8180105",
            "US8189905",
            "US8200011",
            "US8218818",
            "US8218819",
            "US8270732",
            "US8270733",
            "US8280153",
            "US8285046",
            "US8285060",
            "US8295591",
            "US8300924",
            "US8340352",
            "US8358834",
            "US8379085",
            "US8411935",
            "US8416296",
            "US8447096",
            "US8493409",
            "US8494222",
            "US8582882 *",
            "US8620028",
            "US8625884",
            "US8630453 *",
            "US8705861",
            "US8786702",
            "US8797405",
            "US8837774 *",
            "US8941726 *",
            "US20020015048 *",
            "US20040211071 *",
            "US20100303302 *",
            "US20110135158 *",
            "US20110141251 *",
            "US20110205226 *",
            "US20120263373 *",
            "US20140126818 *",
            "US20150078631 *",
            "WO2011022272A2 *",
            "WO2011022275A2 *",
            "WO2013058735A1 *"
        ],
        [
            "Filing date",
            "Oct 26, 1999",
            "Aug 17, 2001",
            "Oct 7, 1999",
            "Sep 5, 2000",
            "Jun 27, 2001",
            "Apr 18, 2002",
            "Aug 7, 2001",
            "May 2, 2005",
            "Apr 25, 2003",
            "Feb 14, 2005",
            "Jun 21, 2007",
            "Aug 31, 2009",
            "Sep 17, 2009",
            "Sep 11, 2008",
            "Sep 17, 2009",
            "Jul 9, 2008",
            "Apr 30, 2008",
            "",
            "Sep 1, 2009",
            "Aug 31, 2009",
            "Aug 31, 2009",
            "Aug 18, 2009",
            "Feb 18, 2009",
            "Aug 31, 2009",
            "Aug 18, 2009",
            "Sep 11, 2008",
            "Aug 18, 2009",
            "Aug 18, 2009",
            "Aug 18, 2009",
            "Jul 9, 2008",
            "Apr 14, 2009",
            "Oct 2, 2008",
            "Aug 18, 2009",
            "May 15, 2012",
            "Jan 23, 2003",
            "Mar 6, 2012",
            "Aug 18, 2009",
            "Dec 2, 2010",
            "Jun 12, 2012",
            "Aug 31, 2009",
            "Aug 31, 2009",
            "May 4, 2011",
            "Dec 10, 2009",
            "Jun 27, 2001",
            "Apr 25, 2003",
            "",
            "",
            "",
            "Oct 21, 2009",
            "May 4, 2011",
            "Nov 6, 2012",
            "Jul 7, 2014",
            "Aug 11, 2010",
            "Aug 11, 2010",
            "Oct 18, 2011"
        ],
        [
            "Publication date",
            "Nov 5, 2002",
            "Sep 9, 2003",
            "Sep 30, 2003",
            "Sep 28, 2004",
            "Mar 15, 2005",
            "Mar 28, 2006",
            "Mar 11, 2008",
            "Jun 3, 2008",
            "Mar 17, 2009",
            "Nov 3, 2009",
            "Dec 6, 2011",
            "May 1, 2012",
            "May 1, 2012",
            "May 8, 2012",
            "May 15, 2012",
            "May 29, 2012",
            "Jun 12, 2012",
            "Jul 10, 2012",
            "Jul 10, 2012",
            "Sep 18, 2012",
            "Sep 18, 2012",
            "Oct 2, 2012",
            "Oct 9, 2012",
            "Oct 9, 2012",
            "Oct 23, 2012",
            "Oct 30, 2012",
            "Dec 25, 2012",
            "Jan 22, 2013",
            "Feb 19, 2013",
            "Apr 2, 2013",
            "Apr 9, 2013",
            "May 21, 2013",
            "Jul 23, 2013",
            "Jul 23, 2013",
            "Nov 12, 2013",
            "Dec 31, 2013",
            "Jan 7, 2014",
            "Jan 14, 2014",
            "Apr 22, 2014",
            "Jul 22, 2014",
            "Aug 5, 2014",
            "Sep 16, 2014",
            "Jan 27, 2015",
            "Feb 7, 2002",
            "Oct 28, 2004",
            "Dec 2, 2010",
            "Jun 9, 2011",
            "Jun 16, 2011",
            "Aug 25, 2011",
            "Oct 18, 2012",
            "May 8, 2014",
            "Mar 19, 2015",
            "Feb 24, 2011",
            "Feb 24, 2011",
            "Apr 25, 2013"
        ],
        [
            "Applicant",
            "Nissan Motor Co., Ltd.",
            "Lmi Technologies Inc.",
            "Ricoh Company, Ltd.",
            "Protrack Ltd.",
            "Telefonaktiebolaget Lm Ericsson (Publ)",
            "Microsoft Corp.",
            "France Telecom",
            "Bitplane Ag",
            "Microsoft Corporation",
            "Sony Corporation",
            "The University Of Southern Mississippi",
            "Behavioral Recognition Systems, Inc.",
            "Behavioral Recognition Systems Inc.",
            "Behavioral Recognition Systems, Inc.",
            "Behavioral Recognition Systems, Inc.",
            "Behavioral Recognition Systems, Inc.",
            "Behavioral Recognition Systems, Inc.",
            "Behavioral Recognition Systems, Inc.",
            "Behavioral Recognition Systems, Inc.",
            "Behavioral Recognition Systems, Inc.",
            "Behavioral Recognition Systems, Inc.",
            "Behavioral Recognition Systems",
            "Behavioral Recognition Systems, Inc.",
            "Behavioral Recognition Systems, Inc.",
            "Behavioral Recognition Systems, Inc.",
            "Behavioral Recognition Systems, Inc.",
            "Behavioral Recognition Systems, Inc.",
            "Behavioral Recognition Systems",
            "Behavioral Recognition Systems, Inc.",
            "Behavioral Recognition Systems, Inc.",
            "Behavioral Recognition Systems, Inc.",
            "Koninklijke Philips Electronics N.V.",
            "Behavioral Recognition Systems, Inc.",
            "Behavioral Recognition Systems, Inc.",
            "Koninklijke Philipse N.V.",
            "Behavioral Recognition Systems, Inc.",
            "Behavioral Recognition Systems, Inc.",
            "Sony Corporation",
            "Behavioral Recognition Systems, Inc.",
            "Behavioral Recognition Systems, Inc.",
            "Behavioral Recognition Systems, Inc.",
            "Bae Systems Information Solutions Inc.",
            "Mitsubishi Electric Research Laboratories, Inc.",
            "David Nister",
            "Microsoft Corporation",
            "Microsoft Corporation",
            "Nishino Katsuaki",
            "Marks Tim K",
            "Koninklijke Philips Electronics N.V.",
            "Bae Systems National Security Solutions Inc.",
            "Sony Corporation",
            "Kriegman-Belhumeur Vision Technologies, Llc",
            "Behavioral Recognition Systems, Inc.",
            "Behavioral Recognition Systems, Inc.",
            "Hewlett-Packard Development Company, L.P."
        ],
        [
            "Title",
            "Position measuring apparatus using a pair of electronic cameras",
            "Method and apparatus for scanning lumber and other objects",
            "Estimation of 3-dimensional shape from image sequence",
            "Real time image registration, motion detection and background replacement using discrete local motion estimation",
            "System and method for median fusion of depth maps",
            "System and method for image-based surface detail transfer",
            "Method for calculating an image interpolated between two images of a video sequence",
            "Computerized method and computer system for positioning a pointer",
            "Estimation of overlap of polygons",
            "Moving object tracking method using occlusion detection of the tracked object, and image processing apparatus",
            "Apparatus and methods for image restoration",
            "Unsupervised learning of temporal anomalies for a video surveillance system",
            "Video surveillance system configured to analyze complex behaviors using alternating layers of clustering and sequencing",
            "Estimator identifier component for behavioral recognition system",
            "Classifier anomalies for observed behaviors in a video surveillance system",
            "Cognitive model for a machine-learning engine in a video analysis system",
            "Context processor for video analysis system",
            "Foreground object tracking",
            "Foreground object detection in a video surveillance system",
            "Clustering nodes in a self-organizing map using an adaptive resonance theory network",
            "Identifying anomalous object types during classification",
            "Visualizing and updating learned trajectories in video surveillance systems",
            "Adaptive update of background pixel thresholds using sudden illumination change detection",
            "Detecting anomalous trajectories in a video surveillance system",
            "Adaptive voting experts for incremental segmentation of sequences with prediction in a video surveillance system",
            "Tracker component for behavioral recognition system",
            "Inter-trajectory anomaly detection using adaptive voting experts in a video surveillance system",
            "Background model for complex and dynamic scenes",
            "Intra-trajectory anomaly detection using adaptive voting experts in a video surveillance system",
            "Semantic representation module of a machine-learning engine in a video analysis system",
            "Mapper component for multiple art networks in a video analysis system",
            "Method and device for processing a depth-map",
            "Visualizing and updating sequences and segments in a video surveillance system",
            "Classifier anomalies for observed behaviors in a video surveillance system",
            "Unit for and method of segmentation using average homogeneity",
            "Behavioral recognition system",
            "Visualizing and updating learned event maps in surveillance systems",
            "Image processing device, image processing method and program",
            "Context processor for video analysis system",
            "Visualizing and updating long-term memory percepts in a video surveillance system",
            "Visualizing and updating classifications in a video surveillance system",
            "Inverse stereo image matching for change detection",
            "Method and system for segmenting moving objects from images using foreground extraction",
            "System and method for median fusion of depth maps",
            "Estimation of overlap of polygons",
            "Systems And Methods For Estimating An Occluded Body Part",
            "Image processing device, image processing method and program",
            "Method and System for Segmenting Moving Objects from Images Using Foreground Extraction",
            "Generation of occlusion data for image properties",
            "Inverse stereo image matching for change detection",
            "Method of occlusion-based background motion estimation",
            "Method and System For Localizing Parts of an Object in an Image For Computer Vision Applications",
            "Scene preset identification using quadtree decomposition analysis",
            "Adaptive voting experts for incremental segmentation of sequences with prediction in a video surveillance system",
            "Depth mask assisted video stabilization"
        ]
    ],
    "pageTitle": "Patent US6252974 - Method and apparatus for depth modelling and providing depth information of ... - Google Patents",
    "title": "",
    "url": "http://www.google.com/patents/US6252974?dq=5,579,517",
    "hasHeader": true,
    "headerPosition": "FIRST_ROW",
    "tableType": "RELATION",
    "tableNum": 7,
    "s3Link": "common-crawl/crawl-data/CC-MAIN-2015-32/segments/1438042989142.82/warc/CC-MAIN-20150728002309-00088-ip-10-236-191-2.ec2.internal.warc.gz",
    "recordEndOffset": 481815519,
    "recordOffset": 481782312,
    "tableOrientation": "HORIZONTAL",
    "TableContextTimeStampAfterTable": "{49439=The above type of representation includes, as a special case, the representation of video as layers as described in \u201cLayered representation for image sequence coding\u201d, J. Y. A. Wang and E. H. Adelson, pp. 221-224, IEEE ICASSP, Vol. 5, Minneapolis, Minn., 1993, which is hereby included by reference. There, the feature points are the recognizable pixels, and the feature point list is the collection of layers., 67135=Alternative methods for handling missing values in bilinear modelling, are given by A. Gifi (1990): Nonlinear muitivariate analysis. J. Wiley & Sons, Chichester UK, chapters 2.4, 3, 4 and 8, and in Lingoes, J. C., Roskam, E. E. and Borg, I. (1979) Geometric representations of relational data, Mathesis Press, Ann Arbor, Michigan, USA, and in Martens and Naes (1989, referenced above) p 158, which are hereby included by reference. Optimal scaling versions of bilinear modelling are described in A. Gifi (1990), Lingoes et al (1979), referenced above, and in \u201cThe principal components of mixed measurement level multivariate data: An alternating least squares method with optimal scaling features\u201d, Young F. W., Takane, Y. and De Leeuw, J., 1979, Psychometrika 43 (2) pp. 279-281, which is hereby included by reference. A slightly different technique is found in \u201cNonmetric common factor analysis: An alternating least squares method with optimal scaling features\u201d, Takane, Y. and Young, F. W., 1979, Behaviormetrica 6, pp. 45-56, which is hereby included by reference., 76702=FIG. 11b shows the basic case of two overlapping objects A 1121 and B 1122 in a frame 1120. They overlap in one region, where it can be found that A is in front of B, A>B. The depth graph 1126 shows object A 1125 in front of object B 1127., 76283=FIG. 11a shows two objects A 1111 and B 1112 that do not overlap in a given frame 1110. The depth graph 1115 for the objects consists only of two isolated nodes 1116, 1117. For many applications, this is valuable information. E.g, in an image decoder, the objects can be decoded independently, without extra processing like Z-buffering, for overlaps., 78660=FIG. 11f shows a frame 1160 with four objects. Object B 1162 occludes object A 1161 and D 1164, while object A 1161 and D 1164 further occlude object C 1163. This is consistent both of the depth orders B>A>D>C and B>D>A>C. This independece between A and D is valuable for the same reasons as explained for FIG. 11a, though the impact from the frontmost object B 1162 on both A 1161 and C 1163 must be considered., 44630=A feature point can be any recognizable part of an image. One possibility is to base feature points on geometrical primitives like coners, as described in \u201cCorner Detection Using Laplacian of Gaussian Operator\u201d, S. Tabbone, \u201cProceedings of The 8th Scandinavian Conference on Image Analysis\u201d, 1993, which is hereby included by reference., 89364=Assume that the partition into objects is based on the first frame 1001, resulting in initial found objects 1015, 1016, 1017 which are yet less than complete because of the occlusions. Still, the found objects can be followed through the frames, resulting in initial positions 1030, 1031, 1032. 1035,1036.1037 for the objects. The parts of the original frames 1001, 1002, 1003 that are not covered by the reconstructions 1021, 1022, 1023 can now be assigned to the found objects 1015, 1016. 1017, e.g. by a method based on the following principle., 33990=In some video modelling techniques it is also advantageous to be able to determine and represent temporal changes in the depth dimension. Present video codec (encoder/decoder) systems do not yield sufficient descriptions of systematic temporal changes in the depth dimension., 75516=The second method for summarizing residuals is based on topological sorting. One object can be seen as a node in a graph. The assumption that object A is in front of object B with a corresponding probability P can be seen as an edge from A to B with a corresponding strength P. This structure can be represented as an incidence matrix, as defined e.g. in \u201cIntroduction to linear algebra\u201d, G. Strang, pp. 339-348, Wellesley-Cambridge Press, 1993, which is hereby included by reference., 72117=Motion estimation for the visible parts of the objects can be performed by some robust method, e.g. as described in Wang & Adelson already mentioned above. Based on this estimation, reconstructions 841, 842, 843, 846, 847, 848, 851, 852, 853, 861, 862, 863 of the silhouettes can be made. For making such reconstructions, the intensities of the objects must be moved, according to their motion estimates and their reference silhouettes. For methods for moving images, see \u201cDigital Image Warping\u201d, third edition, G. Wolberg, IEEE Computer Society Press, 1994, which is hereby included by reference., 77013=FIG. 11c illustrates, in a frame 1130, one object A 1131 that occludes another object B 1132 that again occludes a third object C. 1133 A 1131 occludes C 1133 only indirectly, which is reflected in the depth graph 1136 in that there is no direct edge (arrow) from A 1135 to C 1138., 92483=FIG. 12c illustrates an equivalent data structure. Instead of a occlusion list, having one column for each object and one row for each observation or summary of observations of one occlusion pattern, an occlusion matrix 1250 has one row and one column for each object. A value x in position i,j in an occlusion matrix then means that object i occludes object j with a strength of x, where the strength can be a weight, difference of residuals, probability, confidence etc. as described above. Each element in one occlusion matrix 1260 can then be predicted from corresponding elements of other occlusion matrices 1250,1255., 90794=The innovations assigned to objects by this procedure give rise to occlusions and can therefore be used to estimate depth. To do this, reconstruct the silhouettes of the final objects based on their estimated motion for each frame, find their overlap zones 1060, 1061, 1062, 1065,1066, and proceed as explained for FIG. 8., 54838=Another example of such a Local Occlusion Detector based an motion estimation is the method given in \u201cObtaining a Coherent Motion Field for Motion-Based Segmentation\u201d, D. P. Elias and K. K. Pang, \u201cProceedings of International Picture Coding Symposium\u201d, 1996, Melbourne, Australia, which is hereby included by reference., 93161=The prediction according to FIG. 12b or 12 c can also be done using multivariate techniques. The weights 1235,1240 of FIG. 12b can be modelled by a bilinear model so there is one set of scores for each frame, this or these scores can be predicted, and a prediction for the weights can so be found by multiplying scores by loadings. The method is the same for occlusion matrices as outlined in FIG. 12c, except that the strengths must be formatted into row vectors before the bilinear modelling and reconstruction must be formatted back into a matrix., 91178=Incidence matrices can be forecast, as shown in FIGS. 12a-c. When incidence matrices are known for some frames in a sequence, e.g. the first two frames 1210, 1215 of the sequence shown in FIG. 12a, then an incidence matrix for another, related frame, e.g., the third frame 1220 of the sequence in 12 a. can be made using e.g. linear interpolation, when the frame is between two frame for which the incidence matrix is known, or extrapolation, when the frame follows one or more frames for which the incidence matrix is known. An example of this is given in FIG. 12b. First, one common structure must be chosen for the occlusion list, so that the results from several frame can be compatible. The chosen set of occlusion in the common occlusion list 1230 contains entries for A>B in first row, A>C in second row, and B>C in third row. In this example, the residuals have been used to compute weights for the occlusion; the weights are based on residuals as described above. Weights 1235,1240 for the two first frames 1210, 1215 can so be used to predict, by interpolation or extrapolation, the weights for the next frame. These can so be basis for finding and resolving inconsistencies., 77530=FIG. 11e shows three objects A 1151, B 1152. C 1153 that occlude each other in a pattern that is not consistent with one specific depth order. In the depth graph 1156, this is reflected by a loop. In many real sequences, some of the found occlusions will correspond to physically \u201ccorrect\u201d overlaps, while other found occlusions will come from camera noise, imperfections in motion estimation, or other types of errors. In advance, it is not known which occlusions are \u201ccorrect\u201d and which are \u201cfalse\u201d. In many real examples, the \u201cfalse\u201d occlusions will have a tendency to create inconsistencies in the depth order, corresponding to loops in the depth graph. On the other hand, occlusions corresponding to such errouneous occlusions will often have a tendency to be less systematic over time, or seem to include fewer pixels, or give rise to weaker intensity differences, than the \u201ccorrect\u201d overlaps. It is therefore advantageous to collect all available data about occlusions, and then find which occlusions must be disregarded in order to achieve one consistent depth order., 64741=Both the qualitative occlusion data and the quantiative auxiliary horizontal position data are modelled together in the Multivariate Depth Model Generator. In order to ensure that the horizontal position data properly affect the multivarate total least squares (joint principal component) model, they have been scaled down by a factor 100. Alternatively, PLS2 regression might here have been used (Martens & Naes 1989 already mentioned above)., 66483=It should be noted that other methods than principal component analysis may be used in this invention. For instance, instead of joint principal component analysis one may use a different factor analytic method, such as PLS2 regression with similar iterative missing value estimation. PLS2 regression is described in \u201cMultivariate Calibration\u201d, pp. 146-165, H. Martens and T. Naes, John Wiley & Sons, 1991, which is hereby included by reference. Another alternative is multidimensional scaling (A. Gifi (1990): Nonlinear multivarate analysis. J. Wiley & Sons, Chichester UK, chapter 8, which is hereby included by reference)., 88998=Alternatively, for application where the direction of time is important, refer to FIG. 10. Three frames 1001, 1002, 1003 in a sequence contain three objects 1005, 1006, 1007 that initially occlude each other, but then move away so that new areas of the objects, called innovations, become visible.}",
    "textBeforeTable": "Patent Citations While the invention has been particularity shown and described with reference to the preferred embodiments thereof, it will be understood by those skilled in the art that various changes in form and detail may be made therein without departing from the spirit and scope of the invention. Thus the term \u201cplurality\u201d may also be interpreted in the sense of \u201cone or more\u201d. An encoder can collect incidence matrices corresponding to several frames. These can be aligned, so that occlusions between two given objects always occur in the same positions. It is then possible to perform a bilinear modelling of these matrices, resulting in temporal scores and spatial loadings. The bilinear model does not need to be free of inconsistencies for each frame. This bilinear model can be transmitted from the encoder to a decoder. Inconsistencies in the reconstructed occlusion list entries for each frame, found by multiplying scores by loadings, can then on the decoder side be found and resolved for each individual frame. Another combination of bilinear modelling and inconsistency resolving is the following: The prediction according to FIG. 12b or 12 c can also be done using multivariate techniques. The weights 1235,1240 of FIG. 12b can be modelled by a bilinear model so there is one set of scores for each frame, this or these scores can be predicted, and a prediction for the weights can so be found by multiplying scores by loadings.",
    "textAfterTable": "Apr 25, 2003 Oct 28, 2004 Microsoft Corporation Estimation of overlap of polygons US20100303302 * Dec 2, 2010 Microsoft Corporation Systems And Methods For Estimating An Occluded Body Part US20110135158 * Jun 9, 2011 Nishino Katsuaki Image processing device, image processing method and program US20110141251 * Jun 16, 2011 Marks Tim K Method and System for Segmenting Moving Objects from Images Using Foreground Extraction US20110205226 * Oct 21, 2009 Aug 25, 2011 Koninklijke Philips Electronics N.V. Generation of occlusion data for image properties US20120263373 * May 4, 2011 Oct 18, 2012 Bae Systems National Security Solutions Inc. Inverse stereo image matching for change detection US20140126818 * Nov 6, 2012 May 8, 2014 Sony Corporation Method of occlusion-based background motion estimation US20150078631 * Jul 7, 2014",
    "hasKeyColumn": false,
    "keyColumnIndex": -1,
    "headerRowIndex": 0
}